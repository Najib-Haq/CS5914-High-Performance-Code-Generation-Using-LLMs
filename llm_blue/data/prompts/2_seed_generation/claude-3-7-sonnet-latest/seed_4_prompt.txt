You are an expert in high-performance CUDA programming. Generate a CUDA kernel function that performs a sum reduction on an array of integers.

Implement ONLY the kernel function with this exact signature:
__global__ void sumReduction(int *input, int *output, int size)

The kernel should:
- Take an input array of integers, an output array to store block results, and the size of the input array
- Use shared memory appropriately sized with extern __shared__
- Handle array boundaries correctly using the 'size' parameter
- Use tree-based reduction for high performance
- Use synchronization appropriately
- Aim for the best performance across all input sizes (1K to 1B elements)

Here are previous kernel implementations with their performance metrics:

Implementation 1:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization strategy:
    // 1. Use sequential addressing pattern to avoid bank conflicts in shared memory
    // 2. Process multiple elements per thread during loading phase using grid-stride loops
    // 3. Use thread coarsening - each thread handles multiple elements initially
    // 4. Reduce shared memory transactions with first-level reduction during loading
    // 5. Employ warp shuffle operations for the final warp to eliminate shared memory access and sync barriers
    // 6. Minimize divergent execution with carefully structured conditionals
    // 7. Use compile-time loop unrolling for the last warp reduction
    // 8. Eliminate unnecessary __syncthreads() calls in the final warp
    
    extern __shared__ int sdata[];
    
    // Thread identification
    unsigned int tid = threadIdx.x;
    unsigned int blockSize = blockDim.x;
    unsigned int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int gridSize = blockDim.x * gridDim.x;
    
    // Initialize thread-local sum
    int thread_sum = 0;
    
    // Grid-stride loop to handle arrays of any size
    // Each thread processes multiple elements sequentially, improving coalesced memory access
    for (unsigned int i = globalIdx; i < size; i += gridSize) {
        thread_sum += input[i];
    }
    
    // Store thread sum to shared memory
    sdata[tid] = thread_sum;
    __syncthreads();
    
    // Tree-based reduction in shared memory with sequential addressing
    // Only active threads perform work, reducing warp divergence
    for (unsigned int s = blockSize / 2; s > 32; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    // Final warp reduction using warp-level primitives
    // Eliminates shared memory accesses and __syncthreads() for higher performance
    if (tid < 32) {
        // Use volatile pointer for pre-Volta GPUs that don't have independent thread scheduling
        if (blockSize >= 64) sdata[tid] += sdata[tid + 32];
        
        // Use warp shuffle operations for the final reduction steps
        int value = sdata[tid];
        
        // Unroll the last 5 iterations (warp size = 32, so we need 5 steps: 16,8,4,2,1)
        value += __shfl_down_sync(0xffffffff, value, 16);
        value += __shfl_down_sync(0xffffffff, value, 8);
        value += __shfl_down_sync(0xffffffff, value, 4);
        value += __shfl_down_sync(0xffffffff, value, 2);
        value += __shfl_down_sync(0xffffffff, value, 1);
        
        // First thread in block writes the result to global memory
        if (tid == 0) {
            output[blockIdx.x] = value;
        }
    }
}
```
Performance:
- Size 1024: 15.6826 ms
- Size 1000000: 0.214848 ms
- Size 1000000000: 5.53571 ms

Implementation 2:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization strategy:
    // 1. Sequential addressing to avoid bank conflicts in shared memory
    // 2. Two-element processing per thread during loading to reduce the initial data size
    // 3. Grid-stride loop to handle arbitrary sized inputs efficiently
    // 4. Complete unrolling for last 6 iterations (covers warp size of 32)
    // 5. Use warp shuffle operations for warp-level reductions to eliminate shared memory and sync in final stages
    // 6. Early exit when thread is out of bounds to avoid unnecessary work
    // 7. First add operation during loading to reduce total number of reduction steps
    // 8. Minimize divergent branches by consolidating conditionals
    // 9. Utilize full warp for shuffle operations for maximum throughput
    
    extern __shared__ int sdata[];
    
    // Thread and block identification
    unsigned int tid = threadIdx.x;
    unsigned int blockSize = blockDim.x;
    unsigned int globalIdx = blockIdx.x * blockDim.x * 2 + threadIdx.x; // *2 for loading two elements
    unsigned int gridSize = blockDim.x * gridDim.x * 2; // *2 for the stride
    
    // Each thread accumulates results into a local variable
    int thread_sum = 0;
    
    // Grid-stride loop - each thread processes 2 elements per iteration
    for (unsigned int i = globalIdx; i < size; i += gridSize) {
        thread_sum += input[i];
        
        // Check if the second element is within bounds
        if (i + blockDim.x < size) {
            thread_sum += input[i + blockDim.x];
        }
    }
    
    // Store sum in shared memory
    sdata[tid] = thread_sum;
    __syncthreads();
    
    // Sequential addressing: reduce with fewer bank conflicts and less divergence
    // Only do this for larger reductions, stop when we reach warp size
    for (unsigned int s = blockSize / 2; s > 32; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }
    
    // Last warp reduction using warp shuffle
    if (tid < 32) {
        // Handle the case where blockSize is not a multiple of warp size
        if (blockSize >= 64) {
            sdata[tid] += sdata[tid + 32];
        }
        
        // Use warp shuffle operations for the final reduction (no sync needed)
        // Fully unrolled for maximum performance
        int value = sdata[tid];
        
        // Unroll for all possible warp sizes to ensure the compiler selects optimal code
        value += __shfl_down_sync(0xffffffff, value, 16);
        value += __shfl_down_sync(0xffffffff, value, 8);
        value += __shfl_down_sync(0xffffffff, value, 4);
        value += __shfl_down_sync(0xffffffff, value, 2);
        value += __shfl_down_sync(0xffffffff, value, 1);
        
        // Only first thread writes result for this block
        if (tid == 0) {
            output[blockIdx.x] = value;
        }
    }
}
```
Performance:
- Size 1024: 16.2621 ms
- Size 1000000: 0.238144 ms
- Size 1000000000: 6.8944 ms

IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.

Consider implementing a different strategy such as but not limited to:
- Bank-conflict-free memory access patterns
- Sequential addressing vs. strided addressing
- Warp-level primitives like __shfl_down_sync() for warp-level reductions
- Loop unrolling for the reduction phase
- Early exit strategies to reduce unnecessary work
- Minimizing divergent execution paths

Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).

First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.

The wrapper code will:
- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)
- Handle the final reduction across blocks

Output ONLY the kernel function, starting with __global__ void sumReduction
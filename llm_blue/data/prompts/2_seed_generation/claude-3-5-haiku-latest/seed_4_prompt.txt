You are an expert in high-performance CUDA programming. Generate a CUDA kernel function that performs a sum reduction on an array of integers.

Implement ONLY the kernel function with this exact signature:
__global__ void sumReduction(int *input, int *output, int size)

The kernel should:
- Take an input array of integers, an output array to store block results, and the size of the input array
- Use shared memory appropriately sized with extern __shared__
- Handle array boundaries correctly using the 'size' parameter
- Use tree-based reduction for high performance
- Use synchronization appropriately
- Aim for the best performance across all input sizes (1K to 1B elements)

Here are previous kernel implementations with their performance metrics:

Implementation 1:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization Strategy:
    // 1. Combine shuffle-based and shared memory reduction
    // 2. Use warp-level shuffle instructions for initial reduction
    // 3. Minimize global memory accesses and synchronization
    // 4. Handle non-power-of-2 sizes with predicated load
    // 5. Reduce bank conflicts through sequential addressing
    // 6. Leverage instruction-level parallelism with unrolled reduction
    
    // Declare shared memory for final block-level reduction
    extern __shared__ int sharedData[];
    
    // Global thread indices
    int tid = threadIdx.x;
    int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Predicated load with boundary check
    int value = (globalIdx < size) ? input[globalIdx] : 0;
    
    // Warp-level reduction using shuffle instructions
    // Unrolled for better performance
    #pragma unroll
    for (int offset = warpSize/2; offset > 0; offset /= 2) {
        value += __shfl_down_sync(0xffffffff, value, offset);
    }
    
    // Store warp reduction result for first thread of each warp
    if (tid % warpSize == 0) {
        sharedData[tid / warpSize] = value;
    }
    
    // Block-level synchronization
    __syncthreads();
    
    // Final block reduction using shared memory
    // First warp handles final reduction
    if (tid < warpSize) {
        // Load warp reduction results, handling potential empty warps
        value = (tid < (blockDim.x + warpSize - 1) / warpSize) ? 
                sharedData[tid] : 0;
        
        // Final warp-level shuffle reduction
        #pragma unroll
        for (int offset = warpSize/2; offset > 0; offset /= 2) {
            value += __shfl_down_sync(0xffffffff, value, offset);
        }
        
        // First thread writes block result
        if (tid == 0) {
            output[blockIdx.x] = value;
        }
    }
}
```
Performance:
- Size 1024: 0.243648 ms
- Size 1000000: 0.236384 ms
- Size 1000000000: 5.52813 ms

Implementation 2:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization Strategy:
    // 1. Hybrid reduction approach combining warp shuffle and shared memory
    // 2. Maximize parallel reduction efficiency
    // 3. Minimize global memory accesses and synchronization overhead
    // 4. Handle non-power-of-2 sizes with predicated loading
    // 5. Reduce bank conflicts through careful shared memory layout
    // 6. Leverage instruction-level parallelism with aggressive unrolling
    
    // Declare shared memory with sequential addressing to reduce bank conflicts
    extern __shared__ int sharedData[];
    
    // Thread and index calculations
    int tid = threadIdx.x;
    int globalIdx = blockIdx.x * blockDim.x * 2 + threadIdx.x;
    
    // Cooperative loading with boundary checks and sequential memory access
    int value = 0;
    if (globalIdx < size) {
        value = input[globalIdx];
        // Load two elements per thread to increase computational efficiency
        if (globalIdx + blockDim.x < size) {
            value += input[globalIdx + blockDim.x];
        }
    }
    
    // Initial warp-level reduction using shuffle instructions
    // Unrolled for maximum performance
    #pragma unroll
    for (int offset = warpSize/2; offset > 0; offset /= 2) {
        value += __shfl_down_sync(0xffffffff, value, offset);
    }
    
    // Store warp reduction results in shared memory
    if (tid % warpSize == 0) {
        sharedData[tid / warpSize] = value;
    }
    
    // Block-level synchronization
    __syncthreads();
    
    // Final block reduction using first warp
    if (tid < warpSize) {
        // Load warp reduction results, handling potential empty warps
        value = (tid < (blockDim.x * 2 + warpSize - 1) / warpSize) ? 
                sharedData[tid] : 0;
        
        // Final warp-level shuffle reduction with aggressive unrolling
        #pragma unroll
        for (int offset = warpSize/2; offset > 0; offset /= 2) {
            value += __shfl_down_sync(0xffffffff, value, offset);
        }
        
        // First thread writes block result
        if (tid == 0) {
            output[blockIdx.x] = value;
        }
    }
}
```
Performance:
- Size 1024: 18.1371 ms
- Size 1000000: 0.231328 ms
- Size 1000000000: 7.02672 ms

IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.

Consider implementing a different strategy such as but not limited to:
- Bank-conflict-free memory access patterns
- Sequential addressing vs. strided addressing
- Warp-level primitives like __shfl_down_sync() for warp-level reductions
- Loop unrolling for the reduction phase
- Early exit strategies to reduce unnecessary work
- Minimizing divergent execution paths

Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).

First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.

The wrapper code will:
- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)
- Handle the final reduction across blocks

Output ONLY the kernel function, starting with __global__ void sumReduction
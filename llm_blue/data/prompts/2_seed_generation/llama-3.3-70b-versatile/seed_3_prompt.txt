You are an expert in high-performance CUDA programming. Generate a CUDA kernel function that performs a sum reduction on an array of integers.

Implement ONLY the kernel function with this exact signature:
__global__ void sumReduction(int *input, int *output, int size)

The kernel should:
- Take an input array of integers, an output array to store block results, and the size of the input array
- Use shared memory appropriately sized with extern __shared__
- Handle array boundaries correctly using the 'size' parameter
- Use tree-based reduction for high performance
- Use synchronization appropriately
- Aim for the best performance across all input sizes (1K to 1B elements)

Here are previous kernel implementations with their performance metrics:

Implementation 1:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization strategy:
    // 1. Use a sufficient number of threads per block (threadsPerBlock = 256) to fully utilize the GPU's processing capabilities.
    // 2. Utilize sequential addressing in shared memory to minimize bank conflicts.
    // 3. Implement a warp-level reduction using __shfl_down_sync() to reduce the number of threads participating in the final reduction phase.
    // 4. Use loop unrolling for the reduction phase to minimize the number of iterations.
    // 5. Early exit strategy: if the size of the input array is less than the number of threads per block, only the first 'size' threads participate in the reduction.
    // This strategy should be effective because it reduces memory traffic, optimizes computations, and maximizes utilization of GPU resources.

    extern __shared__ int sharedData[];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;
    int laneId = threadIdx.x % 32; // 32 threads per warp

    // Load data from global memory
    int sum = 0;
    while (idx < size) {
        sum += input[idx];
        idx += stride;
    }

    // Store loaded data in shared memory
    sharedData[threadIdx.x] = sum;
    __syncthreads();

    // Warp-level reduction
    for (int offset = 16; offset > 0; offset /= 2) {
        int neighborVal = __shfl_down_sync(0xffffffff, sharedData[threadIdx.x], offset);
        if (laneId < offset) {
            sharedData[threadIdx.x] += neighborVal;
        }
    }

    // Reduction across warps in the block
    if (threadIdx.x % 32 == 0) {
        int warpId = threadIdx.x / 32;
        for (int i = 1; i < 8; i++) { // 8 warps per block (256 threads / 32 threads per warp)
            if (warpId < i) {
                sharedData[warpId * 32] += sharedData[i * 32];
            }
        }
    }

    // Store the block's result in the output array
    if (threadIdx.x == 0) {
        output[blockIdx.x] = sharedData[0];
    }
}
```
Performance:
- Size 1024: 25.7198 ms

Implementation 2:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization strategy:
    // 1. Use a sufficient number of threads per block to fully utilize the GPU's processing capabilities.
    // 2. Implement a bank-conflict-free memory access pattern in shared memory using a sequential addressing pattern.
    // 3. Utilize warp-level primitives like __shfl_down_sync() for efficient warp-level reductions.
    // 4. Apply an early exit strategy to minimize unnecessary work when the input size is less than the number of threads per block.
    // 5. Employ a combination of warp-level and block-level reductions to minimize the number of threads participating in the final reduction phase.
    // This strategy should be effective because it reduces memory traffic, optimizes computations, and maximizes utilization of GPU resources.

    extern __shared__ int sharedData[];
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // Early exit strategy: If the size of the input array is less than the number of threads per block,
    // only the first 'size' threads participate in the reduction.
    if (idx >= size) return;

    // Load data from global memory
    int sum = 0;
    while (idx < size) {
        sum += input[idx];
        idx += stride;
    }

    // Store loaded data in shared memory using a sequential addressing pattern to minimize bank conflicts
    sharedData[threadIdx.x] = sum;
    __syncthreads();

    // Warp-level reduction using __shfl_down_sync()
    for (int offset = 16; offset > 0; offset /= 2) {
        int neighborVal = __shfl_down_sync(0xffffffff, sharedData[threadIdx.x], offset);
        if (threadIdx.x % 32 < offset) {
            sharedData[threadIdx.x] += neighborVal;
        }
    }

    // Reduction across warps in the block
    if (threadIdx.x % 32 == 0) {
        int warpId = threadIdx.x / 32;
        for (int i = 1; i < 8; i++) { // 8 warps per block (256 threads / 32 threads per warp)
            if (warpId < i) {
                sharedData[warpId * 32] += sharedData[i * 32];
            }
        }
    }

    // Store the block's result in the output array
    if (threadIdx.x == 0) {
        output[blockIdx.x] = sharedData[0];
    }
}
```
Performance:
- Size 1024: 23.5827 ms

IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.

Consider implementing a different strategy such as but not limited to:
- Bank-conflict-free memory access patterns
- Sequential addressing vs. strided addressing
- Warp-level primitives like __shfl_down_sync() for warp-level reductions
- Loop unrolling for the reduction phase
- Early exit strategies to reduce unnecessary work
- Minimizing divergent execution paths

Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).

First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.

The wrapper code will:
- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)
- Handle the final reduction across blocks

Output ONLY the kernel function, starting with __global__ void sumReduction
You are an expert in high-performance CUDA programming. Generate a CUDA kernel function that performs a sum reduction on an array of integers.

Implement ONLY the kernel function with this exact signature:
__global__ void sumReduction(int *input, int *output, int size)

The kernel should:
- Take an input array of integers, an output array to store block results, and the size of the input array
- Use shared memory appropriately sized with extern __shared__
- Handle array boundaries correctly using the 'size' parameter
- Use tree-based reduction for high performance
- Use synchronization appropriately
- Aim for the best performance across all input sizes (1K to 1B elements)

Here are previous kernel implementations with their performance metrics:

Implementation 1:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    /*
    Optimization Strategy:
    1. **Grid-Stride Loop**: Each thread processes multiple elements by iterating over the input array with a stride equal to the total number of threads in the grid. This ensures efficient utilization of all threads, especially for very large arrays (up to 1B elements).
    
    2. **Per-Warp Reduction using Warp-Level Primitives**: Utilize `__shfl_down_sync` to perform reductions within each warp without requiring shared memory or synchronization. This leverages the fast warp shuffle instructions to minimize latency and avoid shared memory bank conflicts.
    
    3. **Shared Memory for Warp Aggregation**: After per-warp reductions, each warp's partial sum is written to shared memory. A single warp then performs a final reduction on these partial sums. This hierarchical reduction approach reduces the number of synchronization points and memory accesses.
    
    4. **Minimized Synchronization**: By confining most reductions to warp-level operations and limiting shared memory usage to a single step, the kernel minimizes the use of `__syncthreads()`, thereby reducing synchronization overhead.
    
    5. **Efficient Memory Access Patterns**: Accesses to global memory are coalesced through the grid-stride loop, ensuring maximum memory bandwidth utilization. Shared memory accesses are also optimized to be bank-conflict-free by aligning partial sums per warp.
    
    6. **Scalable for All Input Sizes**: The combined use of grid-stride loops and hierarchical reductions ensures that the kernel scales efficiently from small (1K) to very large (1B) input sizes without performance degradation.
    
    These optimizations collectively enhance performance by maximizing computational throughput, minimizing memory latency, and reducing synchronization overhead, making the kernel highly efficient across a wide range of input sizes.
    */

    extern __shared__ int sdata[];

    unsigned int tid = threadIdx.x;
    unsigned int global_tid = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int gridSize = blockDim.x * gridDim.x;

    // Each thread computes a partial sum using a grid-stride loop
    int sum = 0;
    for (unsigned int idx = global_tid; idx < size; idx += gridSize) {
        sum += input[idx];
    }

    // Perform per-warp reduction using warp-level primitives
    // Assuming warpSize is 32
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
    }

    // The first thread of each warp writes its partial sum to shared memory
    if ((threadIdx.x % warpSize) == 0) {
        sdata[threadIdx.x / warpSize] = sum;
    }

    __syncthreads();

    // Let the first warp handle the reduction of partial sums in shared memory
    if (threadIdx.x < (blockDim.x / warpSize)) {
        sum = (threadIdx.x < (blockDim.x / warpSize)) ? sdata[threadIdx.x] : 0;
        
        // Reduce the partial sums using warp-level primitives
        for (int offset = warpSize / 2; offset > 0; offset /= 2) {
            sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
        }

        // Write the block's total sum to the output array
        if (threadIdx.x == 0) {
            output[blockIdx.x] = sum;
        }
    }
}
```
Performance:
- Size 1024: 16.4628 ms
- Size 1000000: 0.268224 ms
- Size 1000000000: 5.56582 ms

Implementation 2:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    /*
    Optimization Strategy:
    1. **Loop Unrolling in Grid-Stride Loop**: Each thread processes four elements per loop iteration. This reduces loop overhead and increases instruction-level parallelism, enhancing throughput for large arrays.

    2. **Minimized Shared Memory Usage with Warp-Level Reductions**: By leveraging warp shuffle operations (`__shfl_down_sync`), most of the reduction is performed within warps using registers, minimizing reliance on shared memory and reducing synchronization overhead.

    3. **Efficient Shared Memory Reduction with Unrolled Tree-Based Approach**: After intra-warp reductions, partial sums from each warp are stored in shared memory. The final reduction within shared memory is unrolled to minimize loop overhead and take advantage of parallelism, ensuring fast convergence to the block's total sum.

    4. **Bank-Conflict-Free Shared Memory Access Patterns**: Shared memory indices are accessed in a manner that avoids bank conflicts, maximizing memory throughput and ensuring that multiple threads can access shared memory simultaneously without serialization.

    5. **Early Exit for Out-of-Bounds Threads**: Threads that do not contribute to the final sum (e.g., those beyond the current data range) exit early, reducing unnecessary computations and improving overall efficiency across varying input sizes.

    6. **Sequential Addressing in Reduction Phase**: The reduction within shared memory uses sequential addressing to optimize memory access patterns and take advantage of caching mechanisms, further enhancing performance.

    These combined optimizations ensure that the kernel efficiently handles a wide range of input sizes (from 1K to 1B elements) by maximizing computational throughput, minimizing memory latency, and reducing synchronization overhead. The strategy effectively balances workload distribution, memory access efficiency, and parallel reduction techniques to achieve superior performance.
    */

    extern __shared__ int sdata[];

    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;

    int sum = 0;

    // Unroll the grid-stride loop by a factor of 4
    if (idx < size) {
        sum += input[idx];
        if (idx + blockDim.x < size) sum += input[idx + blockDim.x];
        if (idx + 2 * blockDim.x < size) sum += input[idx + 2 * blockDim.x];
        if (idx + 3 * blockDim.x < size) sum += input[idx + 3 * blockDim.x];
    }

    // Perform intra-warp reduction using warp shuffle operations
    // Assuming warpSize is 32
    #pragma unroll
    for (int offset = warpSize / 2; offset > 0; offset /= 2) {
        sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
    }

    // Write the reduced value of each warp to shared memory
    if (tid % warpSize == 0) {
        sdata[tid / warpSize] = sum;
    }

    __syncthreads();

    // Perform block-level reduction only in the first warp
    if (tid < warpSize) {
        // Load partial sums from shared memory
        sum = (tid < (blockDim.x / warpSize)) ? sdata[tid] : 0;

        // Final reduction within the first warp
        #pragma unroll
        for (int offset = warpSize / 2; offset > 0; offset /= 2) {
            sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
        }

        // Write the block's total sum to the output array
        if (tid == 0) {
            output[blockIdx.x] = sum;
        }
    }
}
```
Performance:
- Size 1024: 19.1949 ms
- Size 1000000: 0.228256 ms
- Size 1000000000: 7.95747 ms

IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.

Consider implementing a different strategy such as but not limited to:
- Bank-conflict-free memory access patterns
- Sequential addressing vs. strided addressing
- Warp-level primitives like __shfl_down_sync() for warp-level reductions
- Loop unrolling for the reduction phase
- Early exit strategies to reduce unnecessary work
- Minimizing divergent execution paths

Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).

First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.

The wrapper code will:
- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)
- Handle the final reduction across blocks

Output ONLY the kernel function, starting with __global__ void sumReduction
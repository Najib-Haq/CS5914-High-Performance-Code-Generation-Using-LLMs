You are an expert in high-performance CUDA programming. Generate a CUDA kernel function that performs a sum reduction on an array of integers.

Implement ONLY the kernel function with this exact signature:
__global__ void sumReduction(int *input, int *output, int size)

The kernel should:
- Take an input array of integers, an output array to store block results, and the size of the input array
- Use shared memory appropriately sized with extern __shared__
- Handle array boundaries correctly using the 'size' parameter
- Use tree-based reduction for high performance
- Use synchronization appropriately
- Aim for the best performance across all input sizes (1K to 1B elements)

Here are previous kernel implementations with their performance metrics:

Implementation 1:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    // Optimization Strategy:
    // 1. Utilize shared memory to store partial sums for each block, reducing global memory accesses.
    // 2. Each thread loads multiple elements (if possible) to maximize memory bandwidth and handle large input sizes.
    // 3. Perform a tree-based reduction within shared memory to efficiently sum the elements.
    // 4. Use synchronization (__syncthreads()) to ensure all threads have loaded their data before starting the reduction.
    // 5. Unroll the last warp to minimize synchronization overhead and take advantage of warp-level parallelism.
    // These optimizations collectively enhance performance across a wide range of input sizes by maximizing memory throughput and minimizing synchronization costs.

    extern __shared__ int sdata[];

    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x * 2 + threadIdx.x;

    // Load elements into shared memory, handling boundary conditions
    int sum = 0;
    if (idx < size)
        sum += input[idx];
    if (idx + blockDim.x < size)
        sum += input[idx + blockDim.x];
    sdata[tid] = sum;
    __syncthreads();

    // Tree-based reduction in shared memory
    for (unsigned int stride = blockDim.x / 2; stride > 32; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }

    // Unroll the last warp
    if (tid < 32) {
        volatile int* vsmem = sdata;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid + 8];
        vsmem[tid] += vsmem[tid + 4];
        vsmem[tid] += vsmem[tid + 2];
        vsmem[tid] += vsmem[tid + 1];
    }

    // Write the block's result to the output array
    if (tid == 0)
        output[blockIdx.x] = sdata[0];
}
```
Performance:
- Size 1024: 14.2184 ms
- Size 1000000: 0.254848 ms
- Size 1000000000: 6.90794 ms

Implementation 2:
```cuda
__global__ void sumReduction(int *input, int *output, int size) {
    /*
    Optimization Strategy:
    1. **Grid-Stride Loop**: Each thread processes multiple elements by looping over the input array with a stride of grid size. This ensures efficient handling of very large arrays (up to 1B elements) without launching excessive blocks.
    2. **Shared Memory Utilization**: Partial sums are stored in shared memory to minimize global memory accesses and leverage fast on-chip memory.
    3. **Loop Unrolling in Reduction**: The reduction within shared memory is unrolled to reduce loop overhead and increase instruction-level parallelism.
    4. **Warp-Level Primitives**: For the final steps of the reduction, `__shfl_down_sync` is used to perform warp-level reductions without requiring synchronization, thus minimizing synchronization overhead.
    5. **Avoiding Bank Conflicts**: Access patterns to shared memory are designed to be coalesced and bank-conflict-free, maximizing memory throughput.
    6. **Early Exit for Small Sizes**: Boundary conditions are handled to ensure that threads do not access out-of-bounds memory, maintaining correctness across all input sizes.
    
    These optimizations collectively enhance performance by maximizing memory bandwidth usage, reducing synchronization costs, and leveraging warp-level parallelism, making the kernel highly efficient for large-scale reductions.
    */

    extern __shared__ int sdata[];

    unsigned int tid = threadIdx.x;
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int stride = blockDim.x * gridDim.x;

    // Initialize the partial sum for this thread
    int sum = 0;

    // Grid-stride loop to accumulate partial sums
    while (idx < size) {
        sum += input[idx];
        idx += stride;
    }

    // Store the partial sum in shared memory
    sdata[tid] = sum;
    __syncthreads();

    // Perform tree-based reduction in shared memory
    // Unroll the reduction loop for better performance
    for (unsigned int s = blockDim.x / 2; s > 32; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    // Use warp-level primitives for the final warp
    if (tid < 32) {
        // Reduce within the warp without synchronization
        sdata[tid] += sdata[tid + 32];
        sdata[tid] += sdata[tid + 16];
        sdata[tid] += sdata[tid + 8];
        sdata[tid] += sdata[tid + 4];
        sdata[tid] += sdata[tid + 2];
        sdata[tid] += sdata[tid + 1];
    }

    // Write the block's result to the output array
    if (tid == 0) {
        output[blockIdx.x] = sdata[0];
    }
}
```
Performance:

IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.

Consider implementing a different strategy such as but not limited to:
- Bank-conflict-free memory access patterns
- Sequential addressing vs. strided addressing
- Warp-level primitives like __shfl_down_sync() for warp-level reductions
- Loop unrolling for the reduction phase
- Early exit strategies to reduce unnecessary work
- Minimizing divergent execution paths

Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).

First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.

The wrapper code will:
- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)
- Handle the final reduction across blocks

Output ONLY the kernel function, starting with __global__ void sumReduction
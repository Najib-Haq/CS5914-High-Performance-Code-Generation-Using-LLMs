You are an expert in high-performance CUDA programming. Generate a CUDA kernel function that performs a sum reduction on an array of integers.

Implement ONLY the kernel function with this exact signature:
__global__ void sumReduction(int *input, int *output, int size)

The kernel should:
- Take an input array of integers, an output array to store block results, and the size of the input array
- Use shared memory appropriately sized with extern __shared__
- Handle array boundaries correctly using the 'size' parameter
- Use tree-based reduction for high performance
- Use synchronization appropriately
- Aim for the best performance across all input sizes (1K to 1B elements)

Here are previous kernel implementations with their performance metrics:

Implementation 1:
```cuda
__global__ void sumReduction(int *input, int *output, int size)
{
    /*
    Optimization Strategy:
    1. **Grid-Stride Loop with Manual Unrolling**: Each thread processes multiple elements in the input array using a grid-stride loop with manual unrolling by a factor of 4. This reduces loop overhead and increases instruction-level parallelism, allowing threads to handle larger portions of the data efficiently.
    
    2. **Memory Coalescing**: Consecutive threads access consecutive memory locations, ensuring coalesced global memory accesses. This maximizes memory bandwidth utilization and reduces memory latency.
    
    3. **Intra-Warp Reduction Using Warp Shuffle Instructions (`__shfl_down_sync`)**: Utilizes warp-level primitives to perform reductions within a warp without relying on shared memory. This minimizes latency and avoids shared memory bank conflicts, leading to faster intra-warp reductions.
    
    4. **Per-Warp Partial Sums Stored in Shared Memory with Padding**: Each warp writes its partial sum to shared memory. Padding can be added if necessary to prevent shared memory bank conflicts, although with modern GPUs and careful access patterns, this is often minimized.
    
    5. **Final Block Reduction Using Warp Shuffle Instructions**: The first warp reads the partial sums from shared memory and performs a final reduction using warp shuffle instructions. This eliminates the need for additional synchronization and leverages the low latency of warp-level operations.
    
    6. **Minimized Synchronization and Divergent Execution**: By structuring the kernel to minimize the number of `__syncthreads()` calls and ensuring that all threads within a warp execute similar instructions, the kernel reduces synchronization overhead and avoids divergent execution paths, enhancing overall performance.
    */

    extern __shared__ int shared_data[];

    // Calculate thread and block indices
    unsigned int tid = threadIdx.x;
    unsigned int blockSize = blockDim.x;
    unsigned int gridSize = blockSize * gridDim.x;

    // Initialize local sum
    int sum = 0;

    // Grid-stride loop with manual unrolling by a factor of 4
    for(unsigned int i = tid; i < size; i += gridSize * 4)
    {
        if(i < size)
            sum += input[i];
        if(i + gridSize < size)
            sum += input[i + gridSize];
        if(i + 2 * gridSize < size)
            sum += input[i + 2 * gridSize];
        if(i + 3 * gridSize < size)
            sum += input[i + 3 * gridSize];
    }

    // Intra-warp reduction using warp shuffle
    for(int offset = warpSize / 2; offset > 0; offset /= 2)
    {
        sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
    }

    // Write the per-warp partial sum to shared memory
    if((tid % warpSize) == 0)
    {
        shared_data[tid / warpSize] = sum;
    }

    // Ensure all partial sums are written to shared memory
    __syncthreads();

    // Only the first warp performs the final reduction
    if(tid < (blockSize / warpSize))
    {
        // Load the partial sums from shared memory
        sum = shared_data[tid];

        // Final reduction using warp shuffle
        for(int offset = warpSize / 2; offset > 0; offset /= 2)
        {
            sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
        }

        // Write the final block sum to the output array
        if(tid == 0)
        {
            output[blockIdx.x] = sum;
        }
    }
}
```
Performance:
- Size 1024: 18.218 ms
- Size 1000000000: 3.70099 ms

Implementation 2:
```cuda
__global__ void sumReduction(int *input, int *output, int size)
{
    /*
    Optimization Strategy:
    1. **Enhanced Grid-Stride Loop with Higher Unrolling Factor**: The kernel employs a grid-stride loop with an unrolling factor of 8. This allows each thread to process eight elements per iteration, significantly reducing loop overhead and increasing instruction-level parallelism, which is particularly beneficial for large input sizes (up to 1B elements).
    
    2. **Coalesced Global Memory Accesses**: By accessing consecutive memory locations in a strided manner, threads within a warp access contiguous memory regions, ensuring coalesced global memory accesses. This maximizes memory bandwidth utilization and minimizes memory latency.
    
    3. **Warp-Level Reduction Using `__shfl_down_sync`**: Intra-warp reductions are performed using warp shuffle operations (`__shfl_down_sync`), which enable threads within a warp to collaboratively reduce their partial sums without the need for shared memory. This approach reduces latency and avoids shared memory bank conflicts.
    
    4. **Per-Warp Partial Sums Stored in Shared Memory**: After intra-warp reductions, each warp's partial sum is stored in shared memory. The shared memory is accessed in a bank-conflict-free manner by mapping one shared memory slot per warp. This layout ensures efficient access patterns and minimizes shared memory bank conflicts.
    
    5. **Final Block-Wide Reduction with Loop Unrolling**: The final reduction of per-warp partial sums in shared memory is performed by the first warp. The reduction loop is manually unrolled to eliminate loop overhead and ensure consistent execution paths across threads, enhancing performance.
    
    6. **Boundary Checks for Correctness**: All memory accesses within the grid-stride loop are guarded with boundary checks using the `size` parameter. This ensures that the kernel correctly handles input arrays of arbitrary sizes, ranging from small (1K) to very large (1B) elements without accessing out-of-bounds memory.
    
    7. **Minimized Synchronization Overhead**: Synchronization primitives (`__syncthreads()`) are used judiciously to coordinate between warps only when necessary (i.e., after writing to shared memory). This minimizes synchronization overhead and avoids unnecessary stalls, contributing to overall performance improvements.
    
    By integrating these optimization techniques, the kernel achieves high throughput and scalability, outperforming previous implementations, especially for large-scale reductions involving up to one billion elements.
    */

    extern __shared__ int shared_data[];

    unsigned int tid = threadIdx.x;
    unsigned int blockSize = blockDim.x;
    unsigned int warpSizeLocal = warpSize;
    unsigned int gridSize = blockSize * gridDim.x;

    // Initialize local sum
    int sum = 0;

    // Grid-stride loop with unrolling factor of 8
    for (unsigned int idx = blockIdx.x * blockSize + tid; idx < size; idx += gridSize * 8)
    {
        if (idx < size) sum += input[idx];
        if (idx + gridSize  < size) sum += input[idx + gridSize];
        if (idx + 2 * gridSize < size) sum += input[idx + 2 * gridSize];
        if (idx + 3 * gridSize < size) sum += input[idx + 3 * gridSize];
        if (idx + 4 * gridSize < size) sum += input[idx + 4 * gridSize];
        if (idx + 5 * gridSize < size) sum += input[idx + 5 * gridSize];
        if (idx + 6 * gridSize < size) sum += input[idx + 6 * gridSize];
        if (idx + 7 * gridSize < size) sum += input[idx + 7 * gridSize];
    }

    // Intra-warp reduction using warp shuffle
    for (int offset = warpSizeLocal / 2; offset > 0; offset /= 2)
    {
        sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
    }

    // Write the per-warp partial sum to shared memory
    if ((tid % warpSizeLocal) == 0)
    {
        shared_data[tid / warpSizeLocal] = sum;
    }

    __syncthreads();

    // Final reduction within the first warp
    if (tid < (blockSize / warpSizeLocal))
    {
        int blockSum = shared_data[tid];

        // Reduce the partial sums from each warp
        for (int offset = warpSizeLocal / 2; offset > 0; offset /= 2)
        {
            blockSum += __shfl_down_sync(0xFFFFFFFF, blockSum, offset);
        }

        // Write the block's result to the output array
        if (tid == 0)
        {
            output[blockIdx.x] = blockSum;
        }
    }
}
```
Performance:
- Size 1024: 20.054 ms
- Size 1000000: 0.215296 ms

IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.

Consider implementing a different strategy such as but not limited to:
- Bank-conflict-free memory access patterns
- Sequential addressing vs. strided addressing
- Warp-level primitives like __shfl_down_sync() for warp-level reductions
- Loop unrolling for the reduction phase
- Early exit strategies to reduce unnecessary work
- Minimizing divergent execution paths

Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).

First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.

The wrapper code will:
- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)
- Handle the final reduction across blocks

Output ONLY the kernel function, starting with __global__ void sumReduction
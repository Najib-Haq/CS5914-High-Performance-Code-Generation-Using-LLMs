```cuda
__global__ void sumReduction(int *input, int *output, int size) {
  // Optimization Strategy:
  // This kernel combines several optimizations for improved performance across various input sizes.  It employs a hybrid approach: a warp-level reduction using __shfl_down_sync() for initial reduction within each warp, followed by a tree-based reduction in shared memory. This minimizes global memory accesses and leverages the efficiency of warp-level primitives. Bank-conflict-free access is also used to optimize shared memory access.  The block size is chosen to be a multiple of the warp size (32) to fully utilize warp-level parallelism.  Early exit conditions are employed to handle cases where the input size is less than the block size.

  const int warpSize = 32;
  extern __shared__ int sdata[];

  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int tid = threadIdx.x;

  // Early exit if input size is smaller than block size.
  if (i >= size) {
    sdata[tid] = 0;
    return;
  }

  // Load data from global memory to shared memory with bank-conflict free access.
  sdata[tid] = input[i + (tid % warpSize) * (blockDim.x / warpSize)];
  __syncthreads();

  // Warp-level reduction using __shfl_down_sync
  int sum = sdata[tid];
  for (int offset = warpSize / 2; offset > 0; offset >>= 1) {
    sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);
  }

  // Only the first thread of each warp writes to shared memory
  if (tid % warpSize == 0) {
    sdata[tid / warpSize] = sum;
  }
  __syncthreads();

  // Tree-based reduction in shared memory
  int numWarps = blockDim.x / warpSize;
  for (int s = numWarps / 2; s > 0; s >>= 1) {
    if (tid / warpSize < s) {
      sdata[tid / warpSize] += sdata[tid / warpSize + s];
    }
    __syncthreads();
  }

  // Write the block sum to global memory
  if (tid / warpSize == 0) {
    output[blockIdx.x] = sdata[0];
  }
}
```

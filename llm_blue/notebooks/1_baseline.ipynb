{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=457258;https://logfire.pydantic.dev/prayash/hpc4llm\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/prayash/hpc4llm\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "import shutil\n",
    "import logfire\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "from great_tables import GT, style, loc\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logfire.configure()\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc found at: /usr/local/cuda-12.4/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PATH\"] = \"/usr/local/cuda-12.4/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.4/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "nvcc_path = shutil.which(\"nvcc\")\n",
    "if nvcc_path is None:\n",
    "    print(\"[ERROR] nvcc not found in PATH. Please ensure that nvcc is installed and its directory is added to the PATH environment variable.\")\n",
    "else:\n",
    "    print(\"nvcc found at:\", nvcc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cuda_code(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the first code block enclosed by triple backticks.\n",
    "    Prefer blocks that start with ```cuda. If none found, fallback to a generic triple-backtick block.\n",
    "    Removes any <think>... or other extraneous text outside code fences.\n",
    "    \"\"\"\n",
    "    text_no_think = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    pattern_cuda = r'```cuda(.*?)```'\n",
    "    match = re.search(pattern_cuda, text_no_think, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if not match:\n",
    "        pattern_generic = r'```(.*?)```'\n",
    "        match = re.search(pattern_generic, text_no_think, flags=re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        code = match.group(1).strip()\n",
    "    else:\n",
    "        code = text_no_think.strip()\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are an expert in high-performance CUDA code generation. Generate a complete and valid CUDA program \"\n",
    "    \"that performs a sum reduction on an array of 1024 integers:\\n\"\n",
    "    \"  - Measure kernel execution time using CUDA events\\n\"\n",
    "    \"  - Print the final sum\\n\"\n",
    "    \"  - Print the kernel execution time\\n\"\n",
    "    \"  - Be self-contained with a main() function\\n\"\n",
    "    \"  - Compile with nvcc (version 12.4)\\n\\n\"\n",
    "    \"Below is a minimal main function skeleton that you may expand upon. You may change it or replace it entirely \"\n",
    "    \"with your own structure, as long as the requirements above are met:\\n\\n\"\n",
    "    \"```\\n\"\n",
    "    \"#include <cuda_runtime.h>\\n\"\n",
    "    \"#include <iostream>\\n\"\n",
    "    \"\\n\"\n",
    "    \"// You may rename or modify this kernel as you see fit.\\n\"\n",
    "    \"__global__ void sumReductionKernel(int *d_input, int *d_output) {\\n\"\n",
    "    \"    // Fill in your sum reduction logic here.\\n\"\n",
    "    \"}\\n\"\n",
    "    \"\\n\"\n",
    "    \"int main() {\\n\"\n",
    "    \"    // 1. Allocate and initialize an array of 1024 integers on the host.\\n\"\n",
    "    \"    // 2. Allocate device memory.\\n\"\n",
    "    \"    // 3. Copy data from host to device.\\n\"\n",
    "    \"    // 4. Create CUDA events to measure execution time.\\n\"\n",
    "    \"    // 5. Launch the sum reduction kernel.\\n\"\n",
    "    \"    // 6. Record the kernel execution time.\\n\"\n",
    "    \"    // 7. Copy the final sum back to the host.\\n\"\n",
    "    \"    // 8. Print the sum and the kernel execution time.\\n\"\n",
    "    \"    // 9. Clean up device memory.\\n\"\n",
    "    \"\\n\"\n",
    "    \"    return 0;\\n\"\n",
    "    \"}\\n\"\n",
    "    \"```\\n\\n\"\n",
    "    \"Produce only a single code block with the final, self-contained program that meets the requirements. \"\n",
    "    \"Output must follow this exact format, with no additional commentary or disclaimers:\\n\\n\"\n",
    "    \"```cuda\\n\"\n",
    "    \"// Your complete high-performance CUDA code here\\n\"\n",
    "    \"```\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents configured:\n",
      " - gpt-4o-mini\n",
      " - o1-mini\n",
      " - claude-3-5-sonnet-latest\n",
      " - claude-3-5-haiku-latest\n",
      " - gemini-1.5-flash\n",
      " - llama-3.3-70b-versatile\n",
      " - qwen-2.5-32b\n"
     ]
    }
   ],
   "source": [
    "generated_dir = Path(\"data/1_benchmark\")\n",
    "generated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "agents = {\n",
    "    # OpenAI models\n",
    "    \"gpt-4o-mini\": Agent(model=OpenAIModel(\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \"o1-mini\": Agent(model=OpenAIModel(\"o1-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \n",
    "    # Anthropic models\n",
    "    \"claude-3-5-sonnet-latest\": Agent(model=AnthropicModel(\"claude-3-5-sonnet-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \"claude-3-5-haiku-latest\": Agent(model=AnthropicModel(\"claude-3-5-haiku-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \n",
    "    # Gemini models\n",
    "    \"gemini-1.5-flash\": Agent(model=GeminiModel(\"gemini-1.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "    #\"gemini-2.0-flash\": Agent(model=GeminiModel(\"gemini-2.0-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "\n",
    "    # Opensource models\n",
    "    \"llama-3.3-70b-versatile\": Agent(model=GroqModel(\"llama-3.3-70b-versatile\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    \"qwen-2.5-32b\": Agent(model=GroqModel(\"qwen-2.5-32b\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    #\"deepseek-r1-distill-qwen-32b\": Agent(model=GroqModel(\"deepseek-r1-distill-qwen-32b\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"Agents configured:\")\n",
    "for key in agents:\n",
    "    print(\" -\", key)\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running baseline sum reduction test for gpt-4o-mini ---\n",
      "19:50:41.167 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:50:41.186   preparing model request params run_step=1\n",
      "19:50:41.187   model request\n",
      "19:50:52.310   handle model response\n",
      "\n",
      "--- Running baseline sum reduction test for o1-mini ---\n",
      "19:50:53.145 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:50:53.146   preparing model request params run_step=1\n",
      "19:50:53.146   model request\n",
      "19:51:07.784   handle model response\n",
      "\n",
      "--- Running baseline sum reduction test for claude-3-5-sonnet-latest ---\n",
      "19:51:08.611 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:51:08.612   preparing model request params run_step=1\n",
      "19:51:08.612   model request\n",
      "19:51:21.716   handle model response\n",
      "\n",
      "--- Running baseline sum reduction test for claude-3-5-haiku-latest ---\n",
      "19:51:22.554 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:51:22.555   preparing model request params run_step=1\n",
      "19:51:22.556   model request\n",
      "19:51:32.206   handle model response\n",
      "\n",
      "--- Running baseline sum reduction test for gemini-1.5-flash ---\n",
      "19:51:33.042 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:51:33.043   preparing model request params run_step=1\n",
      "19:51:33.044   model request\n",
      "19:51:38.605   handle model response\n",
      "[ERROR] Compilation failed for gemini-1.5-flash:\n",
      "In file included from /usr/include/c++/4.8.2/chrono:35:0,\n",
      "                 from data/1_benchmark/baseline_gemini-1.5-flash.cu:3:\n",
      "/usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2: error: #error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\n",
      " #error This file requires compiler and library support for the \\\n",
      "  ^\n",
      "\n",
      "\n",
      "--- Running baseline sum reduction test for llama-3.3-70b-versatile ---\n",
      "19:51:38.704 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:51:38.705   preparing model request params run_step=1\n",
      "19:51:38.705   model request\n",
      "19:51:43.252   handle model response\n",
      "\n",
      "--- Running baseline sum reduction test for qwen-2.5-32b ---\n",
      "19:51:44.117 agent run prompt=You are an expert in high-performance CUDA code generation. Ge...```cuda\n",
      "// Your complete high-performance CUDA code here\n",
      "```\n",
      "\n",
      "\n",
      "19:51:44.118   preparing model request params run_step=1\n",
      "19:51:44.119   model request\n",
      "19:51:45.818   handle model response\n",
      "[ERROR] Compilation failed for qwen-2.5-32b:\n",
      "In file included from /usr/include/c++/4.8.2/chrono:35:0,\n",
      "                 from data/1_benchmark/baseline_qwen-2.5-32b.cu:3:\n",
      "/usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2: error: #error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options.\n",
      " #error This file requires compiler and library support for the \\\n",
      "  ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name, agent in agents.items():\n",
    "    print(f\"\\n--- Running baseline sum reduction test for {model_name} ---\")\n",
    "    try:\n",
    "        # Call the agent with our prompt\n",
    "        response = agent.run_sync(prompt)\n",
    "        code_text = response.data if hasattr(response, \"data\") else response\n",
    "        cleaned_code = extract_cuda_code(code_text)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Code generation failed for {model_name}: {e}\")\n",
    "        cleaned_code = \"\"\n",
    "    \n",
    "    # Save the generated CUDA code\n",
    "    code_file = generated_dir / f\"baseline_{model_name}.cu\"\n",
    "    with open(code_file, \"w\") as f:\n",
    "        f.write(cleaned_code)\n",
    "    \n",
    "    # Compile using nvcc\n",
    "    binary_file = generated_dir / f\"baseline_{model_name}.out\"\n",
    "    compile_cmd = [\"nvcc\", \"-O3\", str(code_file), \"-o\", str(binary_file)]\n",
    "    try:\n",
    "        compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30)\n",
    "        if compile_result.returncode != 0:\n",
    "            print(f\"[ERROR] Compilation failed for {model_name}:\\n{compile_result.stderr}\")\n",
    "            exec_output = \"Compilation Failed\"\n",
    "            compile_status = \"Failure\"\n",
    "        else:\n",
    "            compile_status = \"Success\"\n",
    "            # Run the compiled binary\n",
    "            run_result = subprocess.run([str(binary_file)], capture_output=True, text=True, timeout=30)\n",
    "            exec_output = run_result.stdout.strip() if run_result.stdout else run_result.stderr.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception during compile/run for {model_name}: {e}\")\n",
    "        exec_output = \"Error\"\n",
    "        compile_status = \"Failure\"\n",
    "    \n",
    "    # Record results\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Compilation\": compile_status,\n",
    "        \"Execution Output\": exec_output\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to data/test_data/baseline_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_csv = Path(\"data/test_data/baseline_results.csv\")\n",
    "results_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(results_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Model\", \"Compilation\", \"Execution Output\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"\\nResults saved to {results_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"hujaenvpqt\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#hujaenvpqt table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#hujaenvpqt thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#hujaenvpqt p { margin: 0; padding: 0; }\n",
       " #hujaenvpqt .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #hujaenvpqt .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #hujaenvpqt .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #hujaenvpqt .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #hujaenvpqt .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #hujaenvpqt .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #hujaenvpqt .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #hujaenvpqt .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #hujaenvpqt .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #hujaenvpqt .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #hujaenvpqt .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #hujaenvpqt .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #hujaenvpqt .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #hujaenvpqt .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #hujaenvpqt .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #hujaenvpqt .gt_from_md> :first-child { margin-top: 0; }\n",
       " #hujaenvpqt .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #hujaenvpqt .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #hujaenvpqt .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #hujaenvpqt .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #hujaenvpqt .gt_row_group_first td { border-top-width: 2px; }\n",
       " #hujaenvpqt .gt_row_group_first th { border-top-width: 2px; }\n",
       " #hujaenvpqt .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #hujaenvpqt .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #hujaenvpqt .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #hujaenvpqt .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #hujaenvpqt .gt_left { text-align: left; }\n",
       " #hujaenvpqt .gt_center { text-align: center; }\n",
       " #hujaenvpqt .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #hujaenvpqt .gt_font_normal { font-weight: normal; }\n",
       " #hujaenvpqt .gt_font_bold { font-weight: bold; }\n",
       " #hujaenvpqt .gt_font_italic { font-style: italic; }\n",
       " #hujaenvpqt .gt_super { font-size: 65%; }\n",
       " #hujaenvpqt .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #hujaenvpqt .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model\">Model</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Compilation\">Compilation</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Execution Output\">Execution Output</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gpt-4o-mini</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Final Sum: 1024\n",
       "Kernel Execution Time: 14.9934 ms</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">o1-mini</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Sum: 1024\n",
       "Kernel execution time: 12.6894 ms</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">claude-3-5-sonnet-latest</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Sum: 1024\n",
       "Kernel execution time: 13.5342 ms</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">claude-3-5-haiku-latest</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Final Sum: 1024\n",
       "Kernel Execution Time: 13.3683 ms</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gemini-1.5-flash</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">llama-3.3-70b-versatile</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Final sum: 1024\n",
       "Kernel execution time: 19.4846 milliseconds</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">qwen-2.5-32b</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(results_csv)\n",
    "gt_table = GT(df)\n",
    "gt_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

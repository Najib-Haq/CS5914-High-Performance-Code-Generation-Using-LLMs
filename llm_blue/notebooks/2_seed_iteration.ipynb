{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=783885;https://logfire-us.pydantic.dev/prayash/hpc4llm\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/prayash/hpc4llm\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "import nest_asyncio\n",
    "import shutil\n",
    "import logfire\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from great_tables import GT, style, loc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuration\n",
    "logfire.configure()\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(override=True, dotenv_path='llm_blue/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc found at: /usr/local/cuda-12.4/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA paths\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda-12.4/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.4/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "# Check for NVCC availability\n",
    "nvcc_path = shutil.which(\"nvcc\")\n",
    "if nvcc_path is None:\n",
    "    print(\"[ERROR] nvcc not found in PATH. Please ensure that nvcc is installed and its directory is added to the PATH environment variable.\")\n",
    "else:\n",
    "    print(\"nvcc found at:\", nvcc_path)\n",
    "\n",
    "# Create directory structure for seed generation\n",
    "BASE_DIR = Path(\"llm_blue/data\")\n",
    "SEED_DIR = BASE_DIR / \"2_seed_generation\"\n",
    "SEEDS_RESULTS_DIR = BASE_DIR / \"results\" / \"2_seed_generation\"\n",
    "PROMPTS_DIR = BASE_DIR / \"prompts\" / \"2_seed_generation\"\n",
    "\n",
    "# Input sizes for testing\n",
    "INPUT_SIZES = [1024, 1000000, 1000000000]\n",
    "NUM_SEEDS = 5\n",
    "\n",
    "# Create directories\n",
    "for directory in [SEED_DIR, SEEDS_RESULTS_DIR, PROMPTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory structure for 9 models with 5 seeds each\n",
      "Models: gpt-4o-mini, o1-mini, o3-mini, claude-3-7-sonnet-latest, claude-3-5-sonnet-latest, claude-3-5-haiku-latest, gemini-1.5-flash, llama-3.3-70b-versatile, qwen-2.5-32b\n"
     ]
    }
   ],
   "source": [
    "# Define models to use\n",
    "models = {\n",
    "    # OpenAI models\n",
    "    \"gpt-4o-mini\": Agent(model=OpenAIModel(\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \"o1-mini\": Agent(model=OpenAIModel(\"o1-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \"o3-mini\": Agent(model=OpenAIModel(\"o1-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \n",
    "    # Anthropic models\n",
    "    \"claude-3-7-sonnet-latest\": Agent(model=AnthropicModel(\"claude-3-7-sonnet-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \"claude-3-5-sonnet-latest\": Agent(model=AnthropicModel(\"claude-3-5-sonnet-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \"claude-3-5-haiku-latest\": Agent(model=AnthropicModel(\"claude-3-5-haiku-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \n",
    "    # Gemini models\n",
    "    \"gemini-1.5-flash\": Agent(model=GeminiModel(\"gemini-1.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "    \n",
    "    # Opensource models\n",
    "    \"llama-3.3-70b-versatile\": Agent(model=GroqModel(\"llama-3.3-70b-versatile\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    \"qwen-2.5-32b\": Agent(model=GroqModel(\"qwen-2.5-32b\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "}\n",
    "\n",
    "# Create model and seed specific directories\n",
    "for model_name in models:\n",
    "    model_dir = SEED_DIR / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    for seed_idx in range(NUM_SEEDS):\n",
    "        seed_dir = model_dir / f\"seed_{seed_idx}\"\n",
    "        seed_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for size in INPUT_SIZES:\n",
    "            size_dir = seed_dir / f\"size_{size}\"\n",
    "            size_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Created directory structure for {len(models)} models with {NUM_SEEDS} seeds each\")\n",
    "print(f\"Models: {', '.join(models.keys())}\")\n",
    "# Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cuda_code(text: str) -> str:\n",
    "    \"\"\"Extract CUDA code from LLM response.\"\"\"\n",
    "    # Remove any thinking sections\n",
    "    text_cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Look for code blocks in this order of preference\n",
    "    patterns = [\n",
    "        r'```cuda(.*?)```',\n",
    "        r'```cpp(.*?)```',\n",
    "        r'```c(.*?)```',\n",
    "        r'```(.*?)```'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text_cleaned, flags=re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # If no code blocks found, use the whole text\n",
    "    return text_cleaned.strip()\n",
    "\n",
    "def create_cuda_wrapper(kernel_code: str, kernel_name: str) -> str:\n",
    "    \"\"\"Create a standard CUDA wrapper for the kernel with proper reduction and array size parameter.\"\"\"\n",
    "    # Create a complete, robust CUDA program wrapping the kernel\n",
    "    wrapper_code = f\"\"\"\n",
    "#include <cuda_runtime.h>\n",
    "#include <iostream>\n",
    "#include <cstdlib>\n",
    "\n",
    "// The kernel provided by the LLM\n",
    "{kernel_code}\n",
    "\n",
    "// Host function to perform reduction\n",
    "int sumArray(int* h_input, int size) {{\n",
    "    int *d_input, *d_temp;\n",
    "    \n",
    "    // Allocate device memory\n",
    "    cudaMalloc((void**)&d_input, size * sizeof(int));\n",
    "    \n",
    "    // The size of d_temp is based on the number of blocks we'll launch\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    cudaMalloc((void**)&d_temp, blocksPerGrid * sizeof(int));\n",
    "    \n",
    "    // Copy input data to device\n",
    "    cudaMemcpy(d_input, h_input, size * sizeof(int), cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Create CUDA events for timing\n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Launch kernel with the actual size parameter\n",
    "    cudaEventRecord(start);\n",
    "    {kernel_name}<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_temp, size);\n",
    "    cudaEventRecord(stop);\n",
    "    \n",
    "    // Wait for kernel to finish\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Calculate elapsed time\n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "    \n",
    "    // Copy the block results back to host\n",
    "    int* h_temp = new int[blocksPerGrid];\n",
    "    cudaMemcpy(h_temp, d_temp, blocksPerGrid * sizeof(int), cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Finalize the reduction on CPU (sum the block results)\n",
    "    int sum = 0;\n",
    "    for (int i = 0; i < blocksPerGrid; i++) {{\n",
    "        sum += h_temp[i];\n",
    "    }}\n",
    "    \n",
    "    // Print results\n",
    "    std::cout << \"Sum: \" << sum << std::endl;\n",
    "    std::cout << \"Kernel Execution Time: \" << milliseconds << \" ms\" << std::endl;\n",
    "    \n",
    "    // Clean up\n",
    "    cudaFree(d_input);\n",
    "    cudaFree(d_temp);\n",
    "    delete[] h_temp;\n",
    "    \n",
    "    return sum;\n",
    "}}\n",
    "\n",
    "int main(int argc, char** argv) {{\n",
    "    int size = 1024;  // Default size\n",
    "    \n",
    "    if (argc > 1) {{\n",
    "        size = atoi(argv[1]);\n",
    "    }}\n",
    "    \n",
    "    // Print size for verification\n",
    "    std::cout << \"Running CUDA Reduction for size: \" << size << std::endl;\n",
    "    \n",
    "    // Allocate and initialize host array\n",
    "    int* h_input = new int[size];\n",
    "    for (int i = 0; i < size; i++) {{\n",
    "        h_input[i] = 1;  // Set all elements to 1 for easy verification\n",
    "    }}\n",
    "    \n",
    "    // Run the reduction and get the sum\n",
    "    int result = sumArray(h_input, size);\n",
    "    \n",
    "    // Verify result (should equal the array size since all elements are 1)\n",
    "    bool correct = (result == size);\n",
    "    std::cout << \"Result verification: \" << (correct ? \"PASSED\" : \"FAILED\") << std::endl;\n",
    "    \n",
    "    // Clean up\n",
    "    delete[] h_input;\n",
    "    \n",
    "    return 0;\n",
    "}}\n",
    "\"\"\"\n",
    "    return wrapper_code\n",
    "\n",
    "def compile_and_run(cuda_file: Path, size: int, output_dir: Path) -> dict:\n",
    "    \"\"\"Compile and run a CUDA file, returning basic results without profiling.\"\"\"\n",
    "    results = {\n",
    "        \"compilation_success\": False,\n",
    "        \"run_success\": False,\n",
    "        \"execution_time_ms\": None,\n",
    "        \"sum_value\": None,\n",
    "        \"is_correct\": False\n",
    "    }\n",
    "    \n",
    "    # Compile\n",
    "    binary_path = cuda_file.with_suffix(\".out\")\n",
    "    compile_cmd = [\"nvcc\", \"-O3\", str(cuda_file), \"-o\", str(binary_path), \"-std=c++11\"]\n",
    "    \n",
    "    try:\n",
    "        compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=60)\n",
    "        \n",
    "        if compile_result.returncode != 0:\n",
    "            results[\"error\"] = compile_result.stderr\n",
    "            return results\n",
    "        \n",
    "        results[\"compilation_success\"] = True\n",
    "        \n",
    "        # Run\n",
    "        run_cmd = [str(binary_path), str(size)]\n",
    "        \n",
    "        run_result = subprocess.run(run_cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        # Save output\n",
    "        output_file = output_dir / \"output.txt\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(run_result.stdout)\n",
    "            if run_result.stderr:\n",
    "                f.write(\"\\n\\nSTDERR:\\n\")\n",
    "                f.write(run_result.stderr)\n",
    "        \n",
    "        results[\"run_success\"] = True\n",
    "        \n",
    "        # Extract results\n",
    "        time_match = re.search(r'Kernel Execution Time: ([\\d\\.]+) ms', run_result.stdout)\n",
    "        if time_match:\n",
    "            results[\"execution_time_ms\"] = float(time_match.group(1))\n",
    "        \n",
    "        sum_match = re.search(r'Sum: (\\d+)', run_result.stdout)\n",
    "        if sum_match:\n",
    "            results[\"sum_value\"] = int(sum_match.group(1))\n",
    "            results[\"is_correct\"] = (results[\"sum_value\"] == size)\n",
    "        \n",
    "        # Check for verification result\n",
    "        verify_match = re.search(r'Result verification: (PASSED|FAILED)', run_result.stdout)\n",
    "        if verify_match:\n",
    "            results[\"verification\"] = verify_match.group(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if not results[\"compilation_success\"]:\n",
    "            results[\"error\"] = str(e)\n",
    "        else:\n",
    "            results[\"run_success\"] = False\n",
    "            results[\"error\"] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_kernel(kernel_code: str, model_name: str, seed_idx: int) -> dict:\n",
    "    \"\"\"Evaluate a kernel implementation across different input sizes.\"\"\"\n",
    "    # Extract kernel name\n",
    "    kernel_match = re.search(r'__global__\\s+void\\s+(\\w+)', kernel_code)\n",
    "    if not kernel_match:\n",
    "        return {\"error\": \"Could not extract kernel name\"}\n",
    "    \n",
    "    kernel_name = kernel_match.group(1)\n",
    "    \n",
    "    # Create results dict\n",
    "    evaluation = {\n",
    "        \"model\": model_name,\n",
    "        \"seed\": seed_idx,\n",
    "        \"kernel_code\": kernel_code,\n",
    "        \"kernel_name\": kernel_name,\n",
    "        \"sizes\": {}\n",
    "    }\n",
    "    \n",
    "    # Create wrapper code\n",
    "    wrapper_code = create_cuda_wrapper(kernel_code, kernel_name)\n",
    "    \n",
    "    # Evaluate for each size\n",
    "    for size in INPUT_SIZES:\n",
    "        size_dir = SEED_DIR / model_name / f\"seed_{seed_idx}\" / f\"size_{size}\"\n",
    "        size_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save wrapper code\n",
    "        cuda_file = size_dir / f\"{model_name}_seed{seed_idx}_size{size}.cu\"\n",
    "        with open(cuda_file, \"w\") as f:\n",
    "            f.write(wrapper_code)\n",
    "        \n",
    "        # Compile and run\n",
    "        results = compile_and_run(cuda_file, size, size_dir)\n",
    "        evaluation[\"sizes\"][size] = results\n",
    "    \n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_for_kernel(previous_kernels=None, previous_performance=None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for kernel generation, including context from previous implementations.\n",
    "    \n",
    "    Args:\n",
    "        previous_kernels: List of previously generated kernel codes\n",
    "        previous_performance: Performance data from previous kernels\n",
    "    \"\"\"\n",
    "    base_prompt = (\n",
    "        \"You are an expert in high-performance CUDA programming. Generate a CUDA kernel function \"\n",
    "        \"that performs a sum reduction on an array of integers.\\n\\n\"\n",
    "        \"Implement ONLY the kernel function with this exact signature:\\n\"\n",
    "        \"__global__ void sumReduction(int *input, int *output, int size)\\n\\n\"\n",
    "        \"The kernel should:\\n\"\n",
    "        \"- Take an input array of integers, an output array to store block results, and the size of the input array\\n\"\n",
    "        \"- Use shared memory appropriately sized with extern __shared__\\n\"\n",
    "        \"- Handle array boundaries correctly using the 'size' parameter\\n\"\n",
    "        \"- Use tree-based reduction for high performance\\n\"\n",
    "        \"- Use synchronization appropriately\\n\"\n",
    "        \"- Aim for the best performance across all input sizes (1K to 1B elements)\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Add context from previous implementations if available\n",
    "    if previous_kernels and len(previous_kernels) > 0:\n",
    "        base_prompt += \"Here are previous kernel implementations with their performance metrics:\\n\\n\"\n",
    "        \n",
    "        # Include up to 2 previous implementations as context\n",
    "        for i, (kernel, perf) in enumerate(zip(previous_kernels[-2:], previous_performance[-2:])):\n",
    "            base_prompt += f\"Implementation {i+1}:\\n```cuda\\n{kernel}\\n```\\n\"\n",
    "            \n",
    "            # Include performance information if available\n",
    "            if perf:\n",
    "                base_prompt += \"Performance:\\n\"\n",
    "                for size, data in perf.items():\n",
    "                    if data.get(\"is_correct\", False):\n",
    "                        base_prompt += f\"- Size {size}: {data.get('execution_time_ms', 'N/A')} ms\\n\"\n",
    "            \n",
    "            base_prompt += \"\\n\"\n",
    "        \n",
    "        base_prompt += (\n",
    "            \"IMPORTANT: Analyze the strengths and weaknesses of the previous implementations before designing your approach.\\n\\n\"\n",
    "            \"Consider implementing a different strategy such as but not limited to:\\n\"\n",
    "            \"- Bank-conflict-free memory access patterns\\n\"\n",
    "            \"- Sequential addressing vs. strided addressing\\n\"\n",
    "            \"- Warp-level primitives like __shfl_down_sync() for warp-level reductions\\n\"\n",
    "            \"- Loop unrolling for the reduction phase\\n\"\n",
    "            \"- Early exit strategies to reduce unnecessary work\\n\"\n",
    "            \"- Minimizing divergent execution paths\\n\\n\"\n",
    "            \"Your goal is to create an implementation that performs better than previous ones, especially for large input sizes (1B elements).\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    base_prompt += (\n",
    "        \"First, briefly explain (in comments) your optimization strategy and why you believe it will be effective.\\n\\n\"\n",
    "        \"The wrapper code will:\\n\"\n",
    "        \"- Call your kernel with blocks and threads: sumReduction<<<blocksPerGrid, threadsPerBlock, threadsPerBlock * sizeof(int)>>>(d_input, d_output, size)\\n\"\n",
    "        \"- Handle the final reduction across blocks\\n\\n\"\n",
    "        \"Output ONLY the kernel function, starting with __global__ void sumReduction\"\n",
    "    )\n",
    "    return base_prompt\n",
    "\n",
    "def generate_and_evaluate_seed(model_name: str, agent, seed_idx: int, prompt=None) -> dict:\n",
    "    \"\"\"Generate and evaluate a single seed from a model.\"\"\"\n",
    "    try:\n",
    "        # Generate kernel code\n",
    "        print(f\"Generating seed {seed_idx} for {model_name}...\")\n",
    "        \n",
    "        # Use provided prompt or generate default one\n",
    "        if prompt is None:\n",
    "            prompt = generate_prompt_for_kernel()\n",
    "        \n",
    "        # Generate code\n",
    "        response = agent.run_sync(prompt)\n",
    "        code_text = response.data if hasattr(response, \"data\") else response\n",
    "        \n",
    "        # Extract kernel code\n",
    "        kernel_code = extract_cuda_code(code_text)\n",
    "        \n",
    "        # Save the raw response and extracted kernel\n",
    "        seed_dir = SEED_DIR / model_name / f\"seed_{seed_idx}\"\n",
    "        seed_dir.mkdir(exist_ok=True, parents=True)\n",
    "        \n",
    "        with open(seed_dir / \"raw_response.txt\", \"w\") as f:\n",
    "            f.write(code_text)\n",
    "        \n",
    "        with open(seed_dir / \"kernel_code.cu\", \"w\") as f:\n",
    "            f.write(kernel_code)\n",
    "        \n",
    "        # Evaluate the kernel\n",
    "        evaluation = evaluate_kernel(kernel_code, model_name, seed_idx)\n",
    "        \n",
    "        # Save evaluation\n",
    "        with open(seed_dir / \"evaluation.json\", \"w\") as f:\n",
    "            json.dump(evaluation, f, indent=2)\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating/evaluating seed {seed_idx} for {model_name}: {e}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"seed\": seed_idx,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    \n",
    "def generate_multiple_seeds(model_name, agent, num_seeds=5):\n",
    "    \"\"\"Generate and evaluate multiple seeds for a single model.\"\"\"\n",
    "    print(f\"\\n{'='*50}\\nGenerating {num_seeds} seeds for {model_name}\\n{'='*50}\")\n",
    "    \n",
    "    results = []\n",
    "    previous_kernels = []\n",
    "    previous_performance = []\n",
    "    \n",
    "    for seed_idx in range(num_seeds):\n",
    "        print(f\"\\nGenerating seed {seed_idx+1}/{num_seeds}...\")\n",
    "        \n",
    "        # Generate prompt with context from previous implementations\n",
    "        prompt = generate_prompt_for_kernel(previous_kernels, previous_performance)\n",
    "        \n",
    "        # Save prompt\n",
    "        prompt_dir = PROMPTS_DIR / model_name\n",
    "        prompt_dir.mkdir(exist_ok=True)\n",
    "        with open(prompt_dir / f\"seed_{seed_idx}_prompt.txt\", \"w\") as f:\n",
    "            f.write(prompt)\n",
    "        \n",
    "        # Generate and evaluate seed\n",
    "        evaluation = generate_and_evaluate_seed(model_name, agent, seed_idx, prompt)\n",
    "        results.append(evaluation)\n",
    "        \n",
    "        # Add to previous implementations list\n",
    "        previous_kernels.append(evaluation.get(\"kernel_code\", \"\"))\n",
    "        previous_performance.append(evaluation.get(\"sizes\", {}))\n",
    "        \n",
    "        # Delay to avoid rate limiting\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Save summary of all seeds\n",
    "    with open(SEEDS_RESULTS_DIR / f\"{model_name}_all_seeds.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Running seed generation for model: gpt-4o-mini\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for gpt-4o-mini\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for gpt-4o-mini...\n",
      "01:14:10.020 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:14:10.039   preparing model request params run_step=1\n",
      "01:14:10.040   model request\n",
      "01:14:18.644   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for gpt-4o-mini...\n",
      "01:14:25.457 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:14:25.458   preparing model request params run_step=1\n",
      "01:14:25.459   model request\n",
      "01:14:34.934   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for gpt-4o-mini...\n",
      "01:14:42.058 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:14:42.060   preparing model request params run_step=1\n",
      "01:14:42.060   model request\n",
      "01:14:49.430   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for gpt-4o-mini...\n",
      "01:14:56.498 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:14:56.500   preparing model request params run_step=1\n",
      "01:14:56.500   model request\n",
      "01:15:05.762   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for gpt-4o-mini...\n",
      "01:15:12.680 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:15:12.681   preparing model request params run_step=1\n",
      "01:15:12.682   model request\n",
      "01:15:23.763   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/gpt-4o-mini_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: o1-mini\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for o1-mini\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for o1-mini...\n",
      "01:15:30.674 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:15:30.675   preparing model request params run_step=1\n",
      "01:15:30.675   model request\n",
      "01:15:36.995   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for o1-mini...\n",
      "01:15:43.932 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:15:43.933   preparing model request params run_step=1\n",
      "01:15:43.933   model request\n",
      "01:15:50.554   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for o1-mini...\n",
      "01:15:57.472 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:15:57.475   preparing model request params run_step=1\n",
      "01:15:57.475   model request\n",
      "01:16:32.408   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for o1-mini...\n",
      "01:16:39.699 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:16:39.702   preparing model request params run_step=1\n",
      "01:16:39.702   model request\n",
      "01:16:52.641   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for o1-mini...\n",
      "01:16:59.619 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:16:59.622   preparing model request params run_step=1\n",
      "01:16:59.623   model request\n",
      "01:17:11.926   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/o1-mini_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: o3-mini\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for o3-mini\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for o3-mini...\n",
      "01:17:18.862 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:17:18.862   preparing model request params run_step=1\n",
      "01:17:18.863   model request\n",
      "01:17:27.258   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for o3-mini...\n",
      "01:17:34.328 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:17:34.330   preparing model request params run_step=1\n",
      "01:17:34.330   model request\n",
      "01:17:43.374   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for o3-mini...\n",
      "01:17:50.243 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:17:50.245   preparing model request params run_step=1\n",
      "01:17:50.246   model request\n",
      "01:17:57.761   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for o3-mini...\n",
      "01:18:04.668 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:18:04.670   preparing model request params run_step=1\n",
      "01:18:04.670   model request\n",
      "01:18:14.081   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for o3-mini...\n",
      "01:18:20.995 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:18:20.998   preparing model request params run_step=1\n",
      "01:18:20.998   model request\n",
      "01:18:38.688   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/o3-mini_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: claude-3-7-sonnet-latest\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for claude-3-7-sonnet-latest\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for claude-3-7-sonnet-latest...\n",
      "01:18:45.635 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:18:45.636   preparing model request params run_step=1\n",
      "01:18:45.636   model request\n",
      "01:18:55.126   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for claude-3-7-sonnet-latest...\n",
      "01:19:02.055 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:19:02.057   preparing model request params run_step=1\n",
      "01:19:02.057   model request\n",
      "01:19:11.080   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for claude-3-7-sonnet-latest...\n",
      "01:19:17.954 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:19:17.956   preparing model request params run_step=1\n",
      "01:19:17.956   model request\n",
      "01:19:29.721   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for claude-3-7-sonnet-latest...\n",
      "01:19:36.769 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:19:36.771   preparing model request params run_step=1\n",
      "01:19:36.772   model request\n",
      "01:19:50.139   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for claude-3-7-sonnet-latest...\n",
      "01:19:57.045 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:19:57.048   preparing model request params run_step=1\n",
      "01:19:57.048   model request\n",
      "01:20:11.626   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/claude-3-7-sonnet-latest_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: claude-3-5-sonnet-latest\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for claude-3-5-sonnet-latest\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for claude-3-5-sonnet-latest...\n",
      "01:20:15.470 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:20:15.471   preparing model request params run_step=1\n",
      "01:20:15.471   model request\n",
      "01:20:28.270   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for claude-3-5-sonnet-latest...\n",
      "01:20:35.304 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:20:35.305   preparing model request params run_step=1\n",
      "01:20:35.306   model request\n",
      "01:20:50.109   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for claude-3-5-sonnet-latest...\n",
      "01:20:57.111 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:20:57.113   preparing model request params run_step=1\n",
      "01:20:57.113   model request\n",
      "01:21:11.747   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for claude-3-5-sonnet-latest...\n",
      "01:21:18.748 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:21:18.750   preparing model request params run_step=1\n",
      "01:21:18.750   model request\n",
      "01:21:35.770   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for claude-3-5-sonnet-latest...\n",
      "01:21:42.923 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:21:42.926   preparing model request params run_step=1\n",
      "01:21:42.926   model request\n",
      "01:21:58.069   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/claude-3-5-sonnet-latest_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: claude-3-5-haiku-latest\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for claude-3-5-haiku-latest\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for claude-3-5-haiku-latest...\n",
      "01:22:05.341 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:22:05.342   preparing model request params run_step=1\n",
      "01:22:05.342   model request\n",
      "01:22:15.960   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for claude-3-5-haiku-latest...\n",
      "01:22:22.669 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:22:22.670   preparing model request params run_step=1\n",
      "01:22:22.670   model request\n",
      "01:22:34.303   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for claude-3-5-haiku-latest...\n",
      "01:22:41.045 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:22:41.047   preparing model request params run_step=1\n",
      "01:22:41.047   model request\n",
      "01:22:57.285   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for claude-3-5-haiku-latest...\n",
      "01:23:04.129 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:23:04.131   preparing model request params run_step=1\n",
      "01:23:04.131   model request\n",
      "01:23:17.518   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for claude-3-5-haiku-latest...\n",
      "01:23:24.251 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:23:24.253   preparing model request params run_step=1\n",
      "01:23:24.254   model request\n",
      "01:23:38.136   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/claude-3-5-haiku-latest_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: gemini-1.5-flash\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for gemini-1.5-flash\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for gemini-1.5-flash...\n",
      "01:23:44.993 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:23:44.995   preparing model request params run_step=1\n",
      "01:23:44.995   model request\n",
      "01:23:47.426   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for gemini-1.5-flash...\n",
      "01:23:54.139 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:23:54.141   preparing model request params run_step=1\n",
      "01:23:54.141   model request\n",
      "01:23:57.514   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for gemini-1.5-flash...\n",
      "01:24:04.210 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:24:04.212   preparing model request params run_step=1\n",
      "01:24:04.212   model request\n",
      "01:24:08.073   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for gemini-1.5-flash...\n",
      "01:24:14.796 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:24:14.798   preparing model request params run_step=1\n",
      "01:24:14.798   model request\n",
      "01:24:18.683   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for gemini-1.5-flash...\n",
      "01:24:25.377 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:24:25.379   preparing model request params run_step=1\n",
      "01:24:25.379   model request\n",
      "01:24:29.217   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/gemini-1.5-flash_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: llama-3.3-70b-versatile\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for llama-3.3-70b-versatile\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for llama-3.3-70b-versatile...\n",
      "01:24:35.911 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:24:35.912   preparing model request params run_step=1\n",
      "01:24:35.913   model request\n",
      "01:24:37.564   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for llama-3.3-70b-versatile...\n",
      "01:24:44.550 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:24:44.552   preparing model request params run_step=1\n",
      "01:24:44.552   model request\n",
      "01:24:46.753   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for llama-3.3-70b-versatile...\n",
      "01:24:53.667 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:24:53.669   preparing model request params run_step=1\n",
      "01:24:53.669   model request\n",
      "01:24:56.045   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for llama-3.3-70b-versatile...\n",
      "01:25:02.962 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:25:02.965   preparing model request params run_step=1\n",
      "01:25:02.965   model request\n",
      "01:25:05.652   handle model response\n",
      "\n",
      "Generating seed 5/5...\n",
      "Generating seed 4 for llama-3.3-70b-versatile...\n",
      "01:25:12.581 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:25:12.583   preparing model request params run_step=1\n",
      "01:25:12.583   model request\n",
      "01:25:16.804   handle model response\n",
      "Results saved to llm_blue/data/results/2_seed_generation/llama-3.3-70b-versatile_all_seeds.json\n",
      "\n",
      "================================================================================\n",
      "Running seed generation for model: qwen-2.5-32b\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "Generating 5 seeds for qwen-2.5-32b\n",
      "==================================================\n",
      "\n",
      "Generating seed 1/5...\n",
      "Generating seed 0 for qwen-2.5-32b...\n",
      "01:28:13.838 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:28:13.839   preparing model request params run_step=1\n",
      "01:28:13.839   model request\n",
      "01:28:16.084   handle model response\n",
      "\n",
      "Generating seed 2/5...\n",
      "Generating seed 1 for qwen-2.5-32b...\n",
      "01:28:22.777 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:28:22.778   preparing model request params run_step=1\n",
      "01:28:22.779   model request\n",
      "01:28:25.603   handle model response\n",
      "\n",
      "Generating seed 3/5...\n",
      "Generating seed 2 for qwen-2.5-32b...\n",
      "01:28:32.311 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:28:32.313   preparing model request params run_step=1\n",
      "01:28:32.313   model request\n",
      "01:28:35.409   handle model response\n",
      "\n",
      "Generating seed 4/5...\n",
      "Generating seed 3 for qwen-2.5-32b...\n",
      "01:28:42.115 agent run prompt=You are an expert in high-performance CUDA programming. Genera...he kernel function, starting with __global__ void sumReduction\n",
      "01:28:42.117   preparing model request params run_step=1\n",
      "01:28:42.117   model request\n",
      "01:28:44.842   handle model response\n"
     ]
    }
   ],
   "source": [
    "results_by_model = {}\n",
    "\n",
    "# Loop through all models\n",
    "for model_name, agent in models.items():\n",
    "    print(f\"\\n{'='*80}\\nRunning seed generation for model: {model_name}\\n{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate seeds for this model\n",
    "        model_seeds = generate_multiple_seeds(model_name, agent, 5)\n",
    "        results_by_model[model_name] = model_seeds\n",
    "        \n",
    "        # Save model results\n",
    "        results_path = SEEDS_RESULTS_DIR / f\"{model_name}_all_seeds.json\"\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(model_seeds, f, indent=2)\n",
    "            \n",
    "        print(f\"Results saved to {results_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running model {model_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

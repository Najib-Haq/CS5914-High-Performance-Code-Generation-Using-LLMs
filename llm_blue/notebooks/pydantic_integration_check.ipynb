{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_dotenv result: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=193222;https://logfire-us.pydantic.dev/prayash/hpc4llm\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/prayash/hpc4llm\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "import shutil\n",
    "import logfire\n",
    "import csv\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "from great_tables import GT, style, loc\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logfire.configure()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "result = load_dotenv(dotenv_path='llm_blue/.env')\n",
    "print(f\"load_dotenv result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rag/testing/CS5914-High-Performance-Code-Generation-Using-LLMs\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc found at: /usr/local/cuda-12.4/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PATH\"] = \"/usr/local/cuda-12.4/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.4/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "nvcc_path = shutil.which(\"nvcc\")\n",
    "if nvcc_path is None:\n",
    "    print(\"[ERROR] nvcc not found in PATH. Please ensure that nvcc is installed and its directory is added to the PATH environment variable.\")\n",
    "else:\n",
    "    print(\"nvcc found at:\", nvcc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_generated_code(text: str) -> str:\n",
    "    \"\"\"Remove markdown code fences from generated code.\"\"\"\n",
    "    cleaned = text.strip()\n",
    "    if cleaned.startswith(\"```cuda\"):\n",
    "        lines = cleaned.splitlines()\n",
    "        if lines[0].strip().startswith(\"```cuda\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        cleaned = \"\\n\".join(lines).strip()\n",
    "    elif cleaned.startswith(\"```\"):\n",
    "        lines = cleaned.splitlines()\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        cleaned = \"\\n\".join(lines).strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are an expert CUDA code generator. Generate a complete and valid CUDA program that launches a kernel where each thread prints:\\n\"\n",
    "    \"\\\"Hello from CUDA thread X!\\\" (where X is the thread index).\\n\"\n",
    "    \"The program must be self-contained, include a main() function, and be compilable with nvcc (version 12.4, Build cuda_12.4.r12.4/compiler.34097967_0).\\n\\n\"\n",
    "    \"Output must follow this exact format, with no additional commentary or instructions:\\n\\n\"\n",
    "    \"```cuda\\n\"\n",
    "    \"// Your complete CUDA code here\\n\"\n",
    "    \"```\\n\\n\"\n",
    "    \"Output only the code block as shown above.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGroqError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m generated_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/generated_cuda/integration_check\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m generated_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m agents \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# OpenAI models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39mOpenAIModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo1-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39mOpenAIModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo1-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))),\n\u001b[1;32m      8\u001b[0m     \n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Anthropic models\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-sonnet-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39mAnthropicModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-sonnet-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANTHROPIC_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-haiku-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39mAnthropicModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-5-haiku-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANTHROPIC_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))),\n\u001b[1;32m     12\u001b[0m     \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Gemini models\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39mGeminiModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEMINI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#\"gemini-2.0-flash\": Agent(model=GeminiModel(\"gemini-2.0-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Opensource models\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.3-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39m\u001b[43mGroqModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.3-70b-versatile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGROQ_API_KEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen-2.5-32b\u001b[39m\u001b[38;5;124m\"\u001b[39m: Agent(model\u001b[38;5;241m=\u001b[39mGroqModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen-2.5-32b\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#\"deepseek-r1-distill-qwen-32b\": Agent(model=GroqModel(\"deepseek-r1-distill-qwen-32b\", api_key=os.getenv(\"GROQ_API_KEY\")),\u001b[39;00m\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgents configured:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m agents:\n",
      "File \u001b[0;32m~/miniconda3/envs/hpc_gen/lib/python3.12/site-packages/pydantic_ai/models/groq.py:121\u001b[0m, in \u001b[0;36mGroqModel.__init__\u001b[0;34m(self, model_name, api_key, groq_client, http_client)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m AsyncGroq(api_key\u001b[38;5;241m=\u001b[39mapi_key, http_client\u001b[38;5;241m=\u001b[39mhttp_client)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mAsyncGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_async_http_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hpc_gen/lib/python3.12/site-packages/groq/_client.py:262\u001b[0m, in \u001b[0;36mAsyncGroq.__init__\u001b[0;34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    260\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mGroqError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "generated_dir = Path(\"data/generated_cuda/integration_check\")\n",
    "generated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "agents = {\n",
    "    # OpenAI models\n",
    "    \"gpt-4o-mini\": Agent(model=OpenAIModel(\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \"o1-mini\": Agent(model=OpenAIModel(\"o1-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \n",
    "    # Anthropic models\n",
    "    \"claude-3-5-sonnet-latest\": Agent(model=AnthropicModel(\"claude-3-5-sonnet-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \"claude-3-5-haiku-latest\": Agent(model=AnthropicModel(\"claude-3-5-haiku-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \n",
    "    # Gemini models\n",
    "    \"gemini-1.5-flash\": Agent(model=GeminiModel(\"gemini-1.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "    #\"gemini-2.0-flash\": Agent(model=GeminiModel(\"gemini-2.0-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "\n",
    "    # Opensource models\n",
    "    \"llama-3.3-70b-versatile\": Agent(model=GroqModel(\"llama-3.3-70b-versatile\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    \"qwen-2.5-32b\": Agent(model=GroqModel(\"qwen-2.5-32b\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    #\"deepseek-r1-distill-qwen-32b\": Agent(model=GroqModel(\"deepseek-r1-distill-qwen-32b\", api_key=os.getenv(\"GROQ_API_KEY\")),\n",
    "}\n",
    "\n",
    "print(\"Agents configured:\")\n",
    "for key in agents:\n",
    "    print(\" -\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running integration test for gpt-4o-mini ---\n",
      "20:07:25.828 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:07:25.828   preparing model request params run_step=1\n",
      "20:07:25.829   model request\n",
      "20:07:28.612   handle model response\n",
      "[ERROR] Compilation failed for gpt-4o-mini:\n",
      "data/generated_cuda/integration_check/integrationTest_gpt-4o-mini.cu(5): error: identifier \"printf\" is undefined\n",
      "      printf(\"Hello from CUDA thread %d!\\n\", threadId);\n",
      "      ^\n",
      "\n",
      "1 error detected in the compilation of \"data/generated_cuda/integration_check/integrationTest_gpt-4o-mini.cu\".\n",
      "\n",
      "\n",
      "--- Running integration test for o1-mini ---\n",
      "20:07:28.841 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:07:28.842   preparing model request params run_step=1\n",
      "20:07:28.842   model request\n",
      "20:07:31.737   handle model response\n",
      "\n",
      "--- Running integration test for claude-3-5-sonnet-latest ---\n",
      "20:07:32.315 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:07:32.316   preparing model request params run_step=1\n",
      "20:07:32.316   model request\n",
      "20:07:34.024   handle model response\n",
      "\n",
      "--- Running integration test for claude-3-5-haiku-latest ---\n",
      "20:07:34.590 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:07:34.591   preparing model request params run_step=1\n",
      "20:07:34.592   model request\n",
      "20:07:37.670   handle model response\n",
      "\n",
      "--- Running integration test for gemini-1.5-flash ---\n",
      "20:07:38.237 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:07:38.237   preparing model request params run_step=1\n",
      "20:07:38.238   model request\n",
      "[ERROR] Code generation failed for gemini-1.5-flash: Unexpected response from gemini 400, body:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": 400,\n",
      "    \"message\": \"API key expired. Please renew the API key.\",\n",
      "    \"status\": \"INVALID_ARGUMENT\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n",
      "        \"reason\": \"API_KEY_INVALID\",\n",
      "        \"domain\": \"googleapis.com\",\n",
      "        \"metadata\": {\n",
      "          \"service\": \"generativelanguage.googleapis.com\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.LocalizedMessage\",\n",
      "        \"locale\": \"en-US\",\n",
      "        \"message\": \"API key expired. Please renew the API key.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "[ERROR] Compilation failed for gemini-1.5-flash:\n",
      "/usr/lib/gcc/x86_64-redhat-linux/4.8.5/../../../../lib64/crt1.o: In function `_start':\n",
      "(.text+0x20): undefined reference to `main'\n",
      "collect2: error: ld returned 1 exit status\n",
      "\n",
      "\n",
      "--- Running integration test for llama-3.3-70b-versatile ---\n",
      "20:07:38.767 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:07:38.768   preparing model request params run_step=1\n",
      "20:07:38.768   model request\n",
      "20:08:10.207   handle model response\n",
      "\n",
      "--- Running integration test for qwen-2.5-32b ---\n",
      "20:08:10.780 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "20:08:10.781   preparing model request params run_step=1\n",
      "20:08:10.781   model request\n",
      "20:08:11.584   handle model response\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name, agent in agents.items():\n",
    "    print(f\"\\n--- Running integration test for {model_name} ---\")\n",
    "    try:\n",
    "        # Call the agent with our prompt\n",
    "        result = agent.run_sync(prompt)\n",
    "        # Extract the generated text (whether structured or plain text)\n",
    "        code = result.data if hasattr(result, \"data\") else result\n",
    "        cleaned_code = clean_generated_code(code)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Code generation failed for {model_name}: {e}\")\n",
    "        cleaned_code = \"\"\n",
    "    \n",
    "    # Save the generated CUDA code to a file\n",
    "    code_file = generated_dir / f\"integrationTest_{model_name}.cu\"\n",
    "    with open(code_file, \"w\") as f:\n",
    "        f.write(cleaned_code)\n",
    "    \n",
    "    # Compile the CUDA code using nvcc\n",
    "    binary_file = generated_dir / f\"integraitonTest_{model_name}.out\"\n",
    "    compile_cmd = [\"nvcc\", \"-O3\", str(code_file), \"-o\", str(binary_file)]\n",
    "    try:\n",
    "        compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30)\n",
    "        if compile_result.returncode != 0:\n",
    "            print(f\"[ERROR] Compilation failed for {model_name}:\\n{compile_result.stderr}\")\n",
    "            exec_output = \"Compilation Failed\"\n",
    "            compile_status = \"Failure\"\n",
    "        else:\n",
    "            compile_status = \"Success\"\n",
    "            # Run the compiled binary\n",
    "            run_result = subprocess.run([str(binary_file)], capture_output=True, text=True, timeout=30)\n",
    "            exec_output = run_result.stdout.strip() if run_result.stdout else run_result.stderr.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception during compile/run for {model_name}: {e}\")\n",
    "        exec_output = \"Error\"\n",
    "        compile_status = \"Failure\"\n",
    "    \n",
    "    # Record the result for this model\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Compilation\": compile_status,\n",
    "        \"Execution Output\": exec_output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to data/test_data/integration_results1.csv\n"
     ]
    }
   ],
   "source": [
    "results_csv = Path(\"data/test_data/integration_results1.csv\")\n",
    "results_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(results_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Model\", \"Compilation\", \"Execution Output\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "print(f\"\\nResults saved to {results_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"mqmlmibxrt\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#mqmlmibxrt table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#mqmlmibxrt thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#mqmlmibxrt p { margin: 0; padding: 0; }\n",
       " #mqmlmibxrt .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #mqmlmibxrt .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #mqmlmibxrt .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #mqmlmibxrt .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #mqmlmibxrt .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #mqmlmibxrt .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #mqmlmibxrt .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #mqmlmibxrt .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #mqmlmibxrt .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #mqmlmibxrt .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #mqmlmibxrt .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #mqmlmibxrt .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #mqmlmibxrt .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #mqmlmibxrt .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #mqmlmibxrt .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #mqmlmibxrt .gt_from_md> :first-child { margin-top: 0; }\n",
       " #mqmlmibxrt .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #mqmlmibxrt .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #mqmlmibxrt .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #mqmlmibxrt .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #mqmlmibxrt .gt_row_group_first td { border-top-width: 2px; }\n",
       " #mqmlmibxrt .gt_row_group_first th { border-top-width: 2px; }\n",
       " #mqmlmibxrt .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #mqmlmibxrt .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #mqmlmibxrt .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #mqmlmibxrt .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #mqmlmibxrt .gt_left { text-align: left; }\n",
       " #mqmlmibxrt .gt_center { text-align: center; }\n",
       " #mqmlmibxrt .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #mqmlmibxrt .gt_font_normal { font-weight: normal; }\n",
       " #mqmlmibxrt .gt_font_bold { font-weight: bold; }\n",
       " #mqmlmibxrt .gt_font_italic { font-style: italic; }\n",
       " #mqmlmibxrt .gt_super { font-size: 65%; }\n",
       " #mqmlmibxrt .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #mqmlmibxrt .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model\">Model</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Compilation\">Compilation</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Execution Output\">Execution Output</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gpt-4o-mini</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 7!\n",
       "Hello from CUDA thread 8!\n",
       "Hello from CUDA thread 9!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">o1-mini</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 160!\n",
       "Hello from CUDA thread 161!\n",
       "Hello from CUDA thread 162!\n",
       "...\n",
       "Hello from CUDA thread 29!\n",
       "Hello from CUDA thread 30!\n",
       "Hello from CUDA thread 31!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">claude-3-5-sonnet-latest</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 29!\n",
       "Hello from CUDA thread 30!\n",
       "Hello from CUDA thread 31!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">claude-3-5-haiku-latest</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 9!\n",
       "Hello from CUDA thread 10!\n",
       "Hello from CUDA thread 11!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gemini-1.5-flash</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">llama-3.3-70b-versatile</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">qwen-2.5-32b</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 192!\n",
       "Hello from CUDA thread 193!\n",
       "Hello from CUDA thread 194!\n",
       "...\n",
       "Hello from CUDA thread 253!\n",
       "Hello from CUDA thread 254!\n",
       "Hello from CUDA thread 255!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">deepseek-r1-distill-qwen-32b</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(results_csv)\n",
    "\n",
    "df[\"Execution Output\"] = df[\"Execution Output\"].apply(\n",
    "    lambda x: \"\\n\".join(str(x).split(\"\\n\")[:3] + [\"...\"] + str(x).split(\"\\n\")[-3:]) \n",
    "    if len(str(x).split(\"\\n\")) > 6 else x\n",
    ")\n",
    "\n",
    "# Display the updated table with summarized output\n",
    "gt_table = GT(df)\n",
    "gt_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=219169;https://logfire.pydantic.dev/prayash/hpc4llm\u001b\\\u001b[4;36mhttps://logfire.pydantic.dev/prayash/hpc4llm\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import nest_asyncio\n",
    "import shutil\n",
    "import logfire\n",
    "import csv\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.models.anthropic import AnthropicModel\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "from pydantic_ai.models.groq import GroqModel\n",
    "\n",
    "from great_tables import GT, style, loc\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "logfire.configure()\n",
    "nest_asyncio.apply()\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc found at: /usr/local/cuda-12.4/bin/nvcc\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PATH\"] = \"/usr/local/cuda-12.4/bin:\" + os.environ.get(\"PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-12.4/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "nvcc_path = shutil.which(\"nvcc\")\n",
    "if nvcc_path is None:\n",
    "    print(\"[ERROR] nvcc not found in PATH. Please ensure that nvcc is installed and its directory is added to the PATH environment variable.\")\n",
    "else:\n",
    "    print(\"nvcc found at:\", nvcc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_generated_code(text: str) -> str:\n",
    "    \"\"\"Remove markdown code fences from generated code.\"\"\"\n",
    "    cleaned = text.strip()\n",
    "    if cleaned.startswith(\"```cuda\"):\n",
    "        lines = cleaned.splitlines()\n",
    "        if lines[0].strip().startswith(\"```cuda\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        cleaned = \"\\n\".join(lines).strip()\n",
    "    elif cleaned.startswith(\"```\"):\n",
    "        lines = cleaned.splitlines()\n",
    "        if lines[0].strip().startswith(\"```\"):\n",
    "            lines = lines[1:]\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        cleaned = \"\\n\".join(lines).strip()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"You are an expert CUDA code generator. Generate a complete and valid CUDA program that launches a kernel where each thread prints:\\n\"\n",
    "    \"\\\"Hello from CUDA thread X!\\\" (where X is the thread index).\\n\"\n",
    "    \"The program must be self-contained, include a main() function, and be compilable with nvcc (version 12.4, Build cuda_12.4.r12.4/compiler.34097967_0).\\n\\n\"\n",
    "    \"Output must follow this exact format, with no additional commentary or instructions:\\n\\n\"\n",
    "    \"```cuda\\n\"\n",
    "    \"// Your complete CUDA code here\\n\"\n",
    "    \"```\\n\\n\"\n",
    "    \"Output only the code block as shown above.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents configured:\n",
      " - gpt-4o-mini\n",
      " - o1-mini\n",
      " - claude-3-5-sonnet-latest\n",
      " - claude-3-5-haiku-latest\n",
      " - gemini-1.5-flash\n",
      " - gemini-2.0-flash\n",
      " - llama-3.3-70b-versatile\n",
      " - qwen-2.5-32b\n",
      " - deepseek-r1-distill-qwen-32b\n"
     ]
    }
   ],
   "source": [
    "generated_dir = Path(\"data/generated_cuda/integration_check\")\n",
    "generated_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "agents = {\n",
    "    # OpenAI models\n",
    "    \"gpt-4o-mini\": Agent(model=OpenAIModel(\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \"o1-mini\": Agent(model=OpenAIModel(\"o1-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))),\n",
    "    \n",
    "    # Anthropic models\n",
    "    \"claude-3-5-sonnet-latest\": Agent(model=AnthropicModel(\"claude-3-5-sonnet-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \"claude-3-5-haiku-latest\": Agent(model=AnthropicModel(\"claude-3-5-haiku-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))),\n",
    "    \n",
    "    # Gemini models\n",
    "    \"gemini-1.5-flash\": Agent(model=GeminiModel(\"gemini-1.5-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "    #\"gemini-2.0-flash\": Agent(model=GeminiModel(\"gemini-2.0-flash\", api_key=os.getenv(\"GEMINI_API_KEY\"))),\n",
    "\n",
    "    # Opensource models\n",
    "    \"llama-3.3-70b-versatile\": Agent(model=GroqModel(\"llama-3.3-70b-versatile\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    \"qwen-2.5-32b\": Agent(model=GroqModel(\"qwen-2.5-32b\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "    #\"deepseek-r1-distill-qwen-32b\": Agent(model=GroqModel(\"deepseek-r1-distill-qwen-32b\", api_key=os.getenv(\"GROQ_API_KEY\"))),\n",
    "}\n",
    "\n",
    "print(\"Agents configured:\")\n",
    "for key in agents:\n",
    "    print(\" -\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running integration test for gpt-4o-mini ---\n",
      "19:29:46.294 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:29:46.294   preparing model request params run_step=1\n",
      "19:29:46.295   model request\n",
      "19:29:47.631   handle model response\n",
      "\n",
      "--- Running integration test for o1-mini ---\n",
      "19:29:48.186 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:29:48.187   preparing model request params run_step=1\n",
      "19:29:48.188   model request\n",
      "19:29:54.404   handle model response\n",
      "\n",
      "--- Running integration test for claude-3-5-sonnet-latest ---\n",
      "19:29:54.969 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:29:54.970   preparing model request params run_step=1\n",
      "19:29:54.970   model request\n",
      "19:29:56.947   handle model response\n",
      "\n",
      "--- Running integration test for claude-3-5-haiku-latest ---\n",
      "19:29:57.487 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:29:57.488   preparing model request params run_step=1\n",
      "19:29:57.488   model request\n",
      "19:30:00.328   handle model response\n",
      "\n",
      "--- Running integration test for gemini-1.5-flash ---\n",
      "19:30:00.895 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:30:00.895   preparing model request params run_step=1\n",
      "19:30:00.896   model request\n",
      "19:30:02.573   handle model response\n",
      "\n",
      "--- Running integration test for gemini-2.0-flash ---\n",
      "19:30:03.143 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:30:03.143   preparing model request params run_step=1\n",
      "19:30:03.144   model request\n",
      "19:30:04.235   handle model response\n",
      "[ERROR] Compilation failed for gemini-2.0-flash:\n",
      "data/generated_cuda/integration_check/integrationTest_gemini-2.0-flash.cu(5): error: identifier \"printf\" is undefined\n",
      "      printf(\"Hello from CUDA thread %d!\\n\", threadId);\n",
      "      ^\n",
      "\n",
      "1 error detected in the compilation of \"data/generated_cuda/integration_check/integrationTest_gemini-2.0-flash.cu\".\n",
      "\n",
      "\n",
      "--- Running integration test for llama-3.3-70b-versatile ---\n",
      "19:30:04.683 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:30:04.683   preparing model request params run_step=1\n",
      "19:30:04.684   model request\n",
      "19:30:05.465   handle model response\n",
      "\n",
      "--- Running integration test for qwen-2.5-32b ---\n",
      "19:30:06.034 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:30:06.034   preparing model request params run_step=1\n",
      "19:30:06.035   model request\n",
      "19:30:06.387   handle model response\n",
      "\n",
      "--- Running integration test for deepseek-r1-distill-qwen-32b ---\n",
      "19:30:06.919 agent run prompt=You are an expert CUDA code generator. Generate a complete and...CUDA code here\n",
      "```\n",
      "\n",
      "Output only the code block as shown above.\n",
      "19:30:06.920   preparing model request params run_step=1\n",
      "19:30:06.920   model request\n",
      "19:30:08.140   handle model response\n",
      "[ERROR] Compilation failed for deepseek-r1-distill-qwen-32b:\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(1): error: expected a declaration\n",
      "  <think>\n",
      "  ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(2): error: extra text after expected end of number\n",
      "  Okay, so the user wants a CUDA program where each thread prints \"Hello from CUDA thread X!\" with X being the thread index. They specified that it should be a complete, self-contained program that compiles with nvcc version 12.4.\n",
      "                                                                                                                                                                                                                                     ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(6): warning #1654-D: too many characters in character literal -- extra leading characters ignored\n",
      "  I'll start by including cuda.h because that's necessary for CUDA functions. Then, in the main function, I'll set up the kernel launch parameters. The grid and block dimensions need to be defined. For simplicity, I'll choose a grid of 2 blocks and each block having 3 threads, so total 6 threads.\n",
      "   ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(6): warning #1654-D: too many characters in character literal -- extra leading characters ignored\n",
      "  I'll start by including cuda.h because that's necessary for CUDA functions. Then, in the main function, I'll set up the kernel launch parameters. The grid and block dimensions need to be defined. For simplicity, I'll choose a grid of 2 blocks and each block having 3 threads, so total 6 threads.\n",
      "                                                                                                           ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(10): warning #1654-D: too many characters in character literal -- extra leading characters ignored\n",
      "  After defining the kernel, in the main function, I'll launch the kernel with the specified grid and block dimensions. It's important to include cudaDeviceSynchronize() to ensure the kernel completes before the program exits, so the output is visible.\n",
      "                                                    ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(12): warning #1654-D: too many characters in character literal -- extra leading characters ignored\n",
      "  I should also handle any potential CUDA errors by checking the return values, but since the user didn't specify error handling, I'll keep it simple for now.\n",
      "                                                                                                       ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(19): error: unrecognized token\n",
      "  ```cuda\n",
      "  ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(19): error: unrecognized token\n",
      "  ```cuda\n",
      "   ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(19): error: unrecognized token\n",
      "  ```cuda\n",
      "    ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(24): error: identifier \"printf\" is undefined\n",
      "      printf(\"Hello from CUDA thread %d!\\n\", tid);\n",
      "      ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(36): error: unrecognized token\n",
      "  ```\n",
      "  ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(36): error: expected a declaration\n",
      "  ```\n",
      "  ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(36): error: unrecognized token\n",
      "  ```\n",
      "   ^\n",
      "\n",
      "data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu(36): error: unrecognized token\n",
      "  ```\n",
      "    ^\n",
      "\n",
      "10 errors detected in the compilation of \"data/generated_cuda/integration_check/integrationTest_deepseek-r1-distill-qwen-32b.cu\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name, agent in agents.items():\n",
    "    print(f\"\\n--- Running integration test for {model_name} ---\")\n",
    "    try:\n",
    "        # Call the agent with our prompt\n",
    "        result = agent.run_sync(prompt)\n",
    "        # Extract the generated text (whether structured or plain text)\n",
    "        code = result.data if hasattr(result, \"data\") else result\n",
    "        cleaned_code = clean_generated_code(code)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Code generation failed for {model_name}: {e}\")\n",
    "        cleaned_code = \"\"\n",
    "    \n",
    "    # Save the generated CUDA code to a file\n",
    "    code_file = generated_dir / f\"integrationTest_{model_name}.cu\"\n",
    "    with open(code_file, \"w\") as f:\n",
    "        f.write(cleaned_code)\n",
    "    \n",
    "    # Compile the CUDA code using nvcc\n",
    "    binary_file = generated_dir / f\"integraitonTest_{model_name}.out\"\n",
    "    compile_cmd = [\"nvcc\", \"-O3\", str(code_file), \"-o\", str(binary_file)]\n",
    "    try:\n",
    "        compile_result = subprocess.run(compile_cmd, capture_output=True, text=True, timeout=30)\n",
    "        if compile_result.returncode != 0:\n",
    "            print(f\"[ERROR] Compilation failed for {model_name}:\\n{compile_result.stderr}\")\n",
    "            exec_output = \"Compilation Failed\"\n",
    "            compile_status = \"Failure\"\n",
    "        else:\n",
    "            compile_status = \"Success\"\n",
    "            # Run the compiled binary\n",
    "            run_result = subprocess.run([str(binary_file)], capture_output=True, text=True, timeout=30)\n",
    "            exec_output = run_result.stdout.strip() if run_result.stdout else run_result.stderr.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception during compile/run for {model_name}: {e}\")\n",
    "        exec_output = \"Error\"\n",
    "        compile_status = \"Failure\"\n",
    "    \n",
    "    # Record the result for this model\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Compilation\": compile_status,\n",
    "        \"Execution Output\": exec_output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to data/test_data/integration_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_csv = Path(\"data/test_data/integration_results.csv\")\n",
    "results_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(results_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"Model\", \"Compilation\", \"Execution Output\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "print(f\"\\nResults saved to {results_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"qhurawtneo\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n",
       "<style>\n",
       "#qhurawtneo table {\n",
       "          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n",
       "          -webkit-font-smoothing: antialiased;\n",
       "          -moz-osx-font-smoothing: grayscale;\n",
       "        }\n",
       "\n",
       "#qhurawtneo thead, tbody, tfoot, tr, td, th { border-style: none; }\n",
       " tr { background-color: transparent; }\n",
       "#qhurawtneo p { margin: 0; padding: 0; }\n",
       " #qhurawtneo .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n",
       " #qhurawtneo .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n",
       " #qhurawtneo .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n",
       " #qhurawtneo .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n",
       " #qhurawtneo .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #qhurawtneo .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #qhurawtneo .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n",
       " #qhurawtneo .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n",
       " #qhurawtneo .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n",
       " #qhurawtneo .gt_column_spanner_outer:first-child { padding-left: 0; }\n",
       " #qhurawtneo .gt_column_spanner_outer:last-child { padding-right: 0; }\n",
       " #qhurawtneo .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n",
       " #qhurawtneo .gt_spanner_row { border-bottom-style: hidden; }\n",
       " #qhurawtneo .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n",
       " #qhurawtneo .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n",
       " #qhurawtneo .gt_from_md> :first-child { margin-top: 0; }\n",
       " #qhurawtneo .gt_from_md> :last-child { margin-bottom: 0; }\n",
       " #qhurawtneo .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n",
       " #qhurawtneo .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n",
       " #qhurawtneo .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n",
       " #qhurawtneo .gt_row_group_first td { border-top-width: 2px; }\n",
       " #qhurawtneo .gt_row_group_first th { border-top-width: 2px; }\n",
       " #qhurawtneo .gt_striped { background-color: rgba(128,128,128,0.05); }\n",
       " #qhurawtneo .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n",
       " #qhurawtneo .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n",
       " #qhurawtneo .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n",
       " #qhurawtneo .gt_left { text-align: left; }\n",
       " #qhurawtneo .gt_center { text-align: center; }\n",
       " #qhurawtneo .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n",
       " #qhurawtneo .gt_font_normal { font-weight: normal; }\n",
       " #qhurawtneo .gt_font_bold { font-weight: bold; }\n",
       " #qhurawtneo .gt_font_italic { font-style: italic; }\n",
       " #qhurawtneo .gt_super { font-size: 65%; }\n",
       " #qhurawtneo .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n",
       " #qhurawtneo .gt_asterisk { font-size: 100%; vertical-align: 0; }\n",
       " \n",
       "</style>\n",
       "<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n",
       "<thead>\n",
       "\n",
       "<tr class=\"gt_col_headings\">\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model\">Model</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Compilation\">Compilation</th>\n",
       "  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Execution Output\">Execution Output</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody class=\"gt_table_body\">\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gpt-4o-mini</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 7!\n",
       "Hello from CUDA thread 8!\n",
       "Hello from CUDA thread 9!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">o1-mini</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 7!\n",
       "Hello from CUDA thread 8!\n",
       "Hello from CUDA thread 9!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">claude-3-5-sonnet-latest</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 29!\n",
       "Hello from CUDA thread 30!\n",
       "Hello from CUDA thread 31!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">claude-3-5-haiku-latest</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 93!\n",
       "Hello from CUDA thread 94!\n",
       "Hello from CUDA thread 95!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gemini-1.5-flash</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 128!\n",
       "Hello from CUDA thread 129!\n",
       "Hello from CUDA thread 130!\n",
       "...\n",
       "Hello from CUDA thread 125!\n",
       "Hello from CUDA thread 126!\n",
       "Hello from CUDA thread 127!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">gemini-2.0-flash</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">llama-3.3-70b-versatile</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 0!\n",
       "Hello from CUDA thread 1!\n",
       "Hello from CUDA thread 2!\n",
       "...\n",
       "Hello from CUDA thread 7!\n",
       "Hello from CUDA thread 8!\n",
       "Hello from CUDA thread 9!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">qwen-2.5-32b</td>\n",
       "    <td class=\"gt_row gt_left\">Success</td>\n",
       "    <td class=\"gt_row gt_left\">Hello from CUDA thread 128!\n",
       "Hello from CUDA thread 129!\n",
       "Hello from CUDA thread 130!\n",
       "...\n",
       "Hello from CUDA thread 125!\n",
       "Hello from CUDA thread 126!\n",
       "Hello from CUDA thread 127!</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td class=\"gt_row gt_left\">deepseek-r1-distill-qwen-32b</td>\n",
       "    <td class=\"gt_row gt_left\">Failure</td>\n",
       "    <td class=\"gt_row gt_left\">Compilation Failed</td>\n",
       "  </tr>\n",
       "</tbody>\n",
       "\n",
       "\n",
       "</table>\n",
       "\n",
       "</div>\n",
       "        "
      ]
     },
     "metadata": {
      "text/html": {
       "text/html": {
        "isolated": true
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(results_csv)\n",
    "\n",
    "df[\"Execution Output\"] = df[\"Execution Output\"].apply(\n",
    "    lambda x: \"\\n\".join(str(x).split(\"\\n\")[:3] + [\"...\"] + str(x).split(\"\\n\")[-3:]) \n",
    "    if len(str(x).split(\"\\n\")) > 6 else x\n",
    ")\n",
    "\n",
    "# Display the updated table with summarized output\n",
    "gt_table = GT(df)\n",
    "gt_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
